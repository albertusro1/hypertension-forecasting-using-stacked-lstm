{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeyIn89HQ-B8"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zD4BJpS3cbVC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from matplotlib import pyplot\n",
        "import time\n",
        "import io\n",
        "\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OcAcizFRCvI"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"datall.csv\").drop(columns=['Unnamed: 0'])\n",
        "df2 = pd.read_csv(\"data_tambahan_ok.csv\")\n",
        "df = pd.concat([df1, df2]).reset_index().drop(columns=['index'])\n",
        "df"
      ],
      "metadata": {
        "id": "GnY1WX-DtWoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "102c8fd8-7a28-417b-c02d-a2198ff7de1d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    rs4341 rs4961 rs4994 rs17249754 rs2681472 rs1800247 rs3781719 rs1799998  \\\n",
              "0       GG     GG     AA         GG        AA         0        AG        GG   \n",
              "1       GG     TT     AA         AG        AG         0        AG        AG   \n",
              "2       GG     TT     AA         AG        AG         0        AG        AG   \n",
              "3       GG     TT     AA         AG        AG        TT        AG        AG   \n",
              "4       CG     GG     GG         AG        AG         0        AA        AG   \n",
              "..     ...    ...    ...        ...       ...       ...       ...       ...   \n",
              "137     CG     GT     AG         GG        AA        TT        AA        AG   \n",
              "138      0     GG     AA          0        AA        TT         0        AA   \n",
              "139     GG     GG     GG         AG        AG        TT        AA        AG   \n",
              "140     GG     GG     GG         AG        AG        TT        AA        AG   \n",
              "141      0     GG     GG          0        AG        TT         0        AG   \n",
              "\n",
              "    rs11191548 rs9266359  ... rs1800206 rs4253778  rs11099098 rs2296545  \\\n",
              "0           TT         0  ...        CC         0           0        CG   \n",
              "1           TT         0  ...        CC         0           0        CG   \n",
              "2           TT         0  ...        CC         0           0        CG   \n",
              "3           TT        CC  ...        CC        GG          GG        CG   \n",
              "4           TT         0  ...        CC         0           0        CC   \n",
              "..         ...       ...  ...       ...       ...         ...       ...   \n",
              "137         TT         0  ...        CC        CG          GG         0   \n",
              "138         TT         0  ...        CC        CG          TG         0   \n",
              "139         CT         0  ...        CC        GG          GT         0   \n",
              "140         TC         0  ...        CC        GG          TG         0   \n",
              "141         TC         0  ...        CC        GG          TG         0   \n",
              "\n",
              "    rs1122608 rs6749447 rs2361159 rs2021783 rs6013382  status  \n",
              "0          GT        GT        CC        CC        CT  normal  \n",
              "1          GG        GT        TT        CC        CT   hyper  \n",
              "2          GG        GT        TT        CC        CT   hyper  \n",
              "3          GG        GT        TT        CC        CT   hyper  \n",
              "4          GT        TT        CC        CC        CT  normal  \n",
              "..        ...       ...       ...       ...       ...     ...  \n",
              "137        GG        TT         T        CC        TT   hyper  \n",
              "138        GG        TT        TC        CC        CC   hyper  \n",
              "139        GG        GT         T        CC        TT  normal  \n",
              "140        GG        TG        TT        CC        TT  normal  \n",
              "141        GG        TG        TT        CC        TT  normal  \n",
              "\n",
              "[142 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d301519c-852f-4e60-97c0-c89e17c23bb1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rs4341</th>\n",
              "      <th>rs4961</th>\n",
              "      <th>rs4994</th>\n",
              "      <th>rs17249754</th>\n",
              "      <th>rs2681472</th>\n",
              "      <th>rs1800247</th>\n",
              "      <th>rs3781719</th>\n",
              "      <th>rs1799998</th>\n",
              "      <th>rs11191548</th>\n",
              "      <th>rs9266359</th>\n",
              "      <th>...</th>\n",
              "      <th>rs1800206</th>\n",
              "      <th>rs4253778</th>\n",
              "      <th>rs11099098</th>\n",
              "      <th>rs2296545</th>\n",
              "      <th>rs1122608</th>\n",
              "      <th>rs6749447</th>\n",
              "      <th>rs2361159</th>\n",
              "      <th>rs2021783</th>\n",
              "      <th>rs6013382</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GG</td>\n",
              "      <td>GG</td>\n",
              "      <td>AA</td>\n",
              "      <td>GG</td>\n",
              "      <td>AA</td>\n",
              "      <td>0</td>\n",
              "      <td>AG</td>\n",
              "      <td>GG</td>\n",
              "      <td>TT</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>CG</td>\n",
              "      <td>GT</td>\n",
              "      <td>GT</td>\n",
              "      <td>CC</td>\n",
              "      <td>CC</td>\n",
              "      <td>CT</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GG</td>\n",
              "      <td>TT</td>\n",
              "      <td>AA</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>0</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>CG</td>\n",
              "      <td>GG</td>\n",
              "      <td>GT</td>\n",
              "      <td>TT</td>\n",
              "      <td>CC</td>\n",
              "      <td>CT</td>\n",
              "      <td>hyper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GG</td>\n",
              "      <td>TT</td>\n",
              "      <td>AA</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>0</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>CG</td>\n",
              "      <td>GG</td>\n",
              "      <td>GT</td>\n",
              "      <td>TT</td>\n",
              "      <td>CC</td>\n",
              "      <td>CT</td>\n",
              "      <td>hyper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GG</td>\n",
              "      <td>TT</td>\n",
              "      <td>AA</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>CC</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>GG</td>\n",
              "      <td>GG</td>\n",
              "      <td>CG</td>\n",
              "      <td>GG</td>\n",
              "      <td>GT</td>\n",
              "      <td>TT</td>\n",
              "      <td>CC</td>\n",
              "      <td>CT</td>\n",
              "      <td>hyper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CG</td>\n",
              "      <td>GG</td>\n",
              "      <td>GG</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>0</td>\n",
              "      <td>AA</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>CC</td>\n",
              "      <td>GT</td>\n",
              "      <td>TT</td>\n",
              "      <td>CC</td>\n",
              "      <td>CC</td>\n",
              "      <td>CT</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>CG</td>\n",
              "      <td>GT</td>\n",
              "      <td>AG</td>\n",
              "      <td>GG</td>\n",
              "      <td>AA</td>\n",
              "      <td>TT</td>\n",
              "      <td>AA</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>CG</td>\n",
              "      <td>GG</td>\n",
              "      <td>0</td>\n",
              "      <td>GG</td>\n",
              "      <td>TT</td>\n",
              "      <td>T</td>\n",
              "      <td>CC</td>\n",
              "      <td>TT</td>\n",
              "      <td>hyper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>0</td>\n",
              "      <td>GG</td>\n",
              "      <td>AA</td>\n",
              "      <td>0</td>\n",
              "      <td>AA</td>\n",
              "      <td>TT</td>\n",
              "      <td>0</td>\n",
              "      <td>AA</td>\n",
              "      <td>TT</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>CG</td>\n",
              "      <td>TG</td>\n",
              "      <td>0</td>\n",
              "      <td>GG</td>\n",
              "      <td>TT</td>\n",
              "      <td>TC</td>\n",
              "      <td>CC</td>\n",
              "      <td>CC</td>\n",
              "      <td>hyper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>GG</td>\n",
              "      <td>GG</td>\n",
              "      <td>GG</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>AA</td>\n",
              "      <td>AG</td>\n",
              "      <td>CT</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>GG</td>\n",
              "      <td>GT</td>\n",
              "      <td>0</td>\n",
              "      <td>GG</td>\n",
              "      <td>GT</td>\n",
              "      <td>T</td>\n",
              "      <td>CC</td>\n",
              "      <td>TT</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>GG</td>\n",
              "      <td>GG</td>\n",
              "      <td>GG</td>\n",
              "      <td>AG</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>AA</td>\n",
              "      <td>AG</td>\n",
              "      <td>TC</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>GG</td>\n",
              "      <td>TG</td>\n",
              "      <td>0</td>\n",
              "      <td>GG</td>\n",
              "      <td>TG</td>\n",
              "      <td>TT</td>\n",
              "      <td>CC</td>\n",
              "      <td>TT</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>0</td>\n",
              "      <td>GG</td>\n",
              "      <td>GG</td>\n",
              "      <td>0</td>\n",
              "      <td>AG</td>\n",
              "      <td>TT</td>\n",
              "      <td>0</td>\n",
              "      <td>AG</td>\n",
              "      <td>TC</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>CC</td>\n",
              "      <td>GG</td>\n",
              "      <td>TG</td>\n",
              "      <td>0</td>\n",
              "      <td>GG</td>\n",
              "      <td>TG</td>\n",
              "      <td>TT</td>\n",
              "      <td>CC</td>\n",
              "      <td>TT</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>142 rows × 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d301519c-852f-4e60-97c0-c89e17c23bb1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d301519c-852f-4e60-97c0-c89e17c23bb1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d301519c-852f-4e60-97c0-c89e17c23bb1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-10942529-f7ab-4b1e-a19d-45bd2336f597\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-10942529-f7ab-4b1e-a19d-45bd2336f597')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-10942529-f7ab-4b1e-a19d-45bd2336f597 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mccVGRxOfKQv"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO6oRGgd1rtg"
      },
      "source": [
        "Mencetak data pertama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hmH_v9hwscVl"
      },
      "outputs": [],
      "source": [
        "# df.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0jwFQsrgvNCt"
      },
      "outputs": [],
      "source": [
        "# df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd5qZ1Hu2RYo"
      },
      "source": [
        "Rename kolom paling akhir menjadi label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "iVky-2012hey"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={df.columns[-1]: 'label'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2x6XKqQ9vNPC"
      },
      "outputs": [],
      "source": [
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"pre_encoding_data.xlsx\")"
      ],
      "metadata": {
        "id": "mo3fDKtrsDHe"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace Karakter Menjadi Numeric (Sum)\n"
      ],
      "metadata": {
        "id": "etnu4K0ouX1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.replace({'A': '0'}, regex=True) # A: 00\n",
        "df = df.replace({'T': '1'}, regex=True) # T: 01\n",
        "df = df.replace({'G': '2'}, regex=True) # G: 10\n",
        "df = df.replace({'C': '3'}, regex=True) # C: 11\n",
        "\n",
        "df = df.replace({'hyper': 1}, regex=True)\n",
        "df = df.replace({'normal': 0}, regex=True)"
      ],
      "metadata": {
        "id": "q9Vusq3sucMm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"post_encoding_data.xlsx\")"
      ],
      "metadata": {
        "id": "mGejFQTzsTyS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "  print(df[col].unique())"
      ],
      "metadata": {
        "id": "R2aHrDHZo9t2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee2ea2e-629a-46db-fda8-ca4b529fdad5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['22' '32' '33' '0' '--']\n",
            "['22' '11' '21' '0' '12' '2']\n",
            "['00' '22' '02' '0']\n",
            "['22' '02' '00' '0' '20' '--']\n",
            "['00' '02' '22' '0']\n",
            "['0' '11' '31' '33' '13' '--' '1']\n",
            "['02' '00' '0' '--' '22']\n",
            "['22' '02' '00' '0' '2']\n",
            "['11' '31' '33' '0' '13' '1']\n",
            "['0' '33' 0]\n",
            "['31' '33' '11' '13' '0' '1' '--']\n",
            "['01' '00' '11' '10' '0' '1']\n",
            "[0]\n",
            "['02' '00' '22' '0' '--']\n",
            "['00' '02' '22' '0']\n",
            "['0' '31' '33' '11' '13']\n",
            "['31' '11' '33' '13' '0' '3']\n",
            "['33' '22' '32' '0' '23' '2']\n",
            "['33' '22' '32' '0' '23' '3']\n",
            "['0' '22' '32' '33' '2' '--']\n",
            "['0' '22' '11' '21' '12' '1']\n",
            "['32' '33' '0' '22' '23']\n",
            "['21' '22' '11' '12' '0' '1']\n",
            "['21' '11' '22' '12' '0' '1']\n",
            "['33' '11' '3' '1' '31' '13' '0']\n",
            "['33' '31' '0' '--' '3']\n",
            "['31' '33' '11' '13' '0' '3']\n",
            "[0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = df.columns[:-1]\n",
        "cols"
      ],
      "metadata": {
        "id": "OtoSQU3_l_6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09448d2-6fa3-4517-9523-c895f616aa86"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['rs4341', 'rs4961', 'rs4994', 'rs17249754', 'rs2681472', 'rs1800247',\n",
              "       'rs3781719', 'rs1799998', 'rs11191548', 'rs9266359', 'rs1458038',\n",
              "       'rs16998073', 'rs10924160', 'rs35444', 'rs17367504', 'rs3865418',\n",
              "       'rs2288774', 'rs6235', 'rs1800206', 'rs4253778', 'rs11099098',\n",
              "       'rs2296545', 'rs1122608', 'rs6749447', 'rs2361159', 'rs2021783',\n",
              "       'rs6013382'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in df.iterrows():\n",
        "  for col in cols:\n",
        "    iterable = str(df.loc[i][col])\n",
        "    df.loc[i, col] = sum(int(x) for x in iterable if x.isdigit())"
      ],
      "metadata": {
        "id": "OD9Pzh5GiU1R"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tnTTjxy3kN6"
      },
      "source": [
        "Menshuffle data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "iDreDp683jcO"
      },
      "outputs": [],
      "source": [
        "df = df.reset_index().drop(columns=['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "V-Clcvl-4OE8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "c12ce62f-b493-4430-b91b-6916bbd48441"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  rs4341 rs4961 rs4994 rs17249754 rs2681472 rs1800247 rs3781719 rs1799998  \\\n",
              "0      4      4      0          4         0         0         2         4   \n",
              "1      4      2      0          2         2         0         2         2   \n",
              "2      4      2      0          2         2         0         2         2   \n",
              "3      4      2      0          2         2         2         2         2   \n",
              "4      5      4      4          2         2         0         0         2   \n",
              "\n",
              "  rs11191548 rs9266359  ... rs1800206 rs4253778  rs11099098 rs2296545  \\\n",
              "0          2         0  ...         6         0           0         5   \n",
              "1          2         0  ...         6         0           0         5   \n",
              "2          2         0  ...         6         0           0         5   \n",
              "3          2         6  ...         6         4           4         5   \n",
              "4          2         0  ...         6         0           0         6   \n",
              "\n",
              "  rs1122608 rs6749447 rs2361159 rs2021783 rs6013382 label  \n",
              "0         3         3         6         6         4     0  \n",
              "1         4         3         2         6         4     1  \n",
              "2         4         3         2         6         4     1  \n",
              "3         4         3         2         6         4     1  \n",
              "4         3         2         6         6         4     0  \n",
              "\n",
              "[5 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06ef20fc-9464-4cec-ab80-1bffd0023060\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rs4341</th>\n",
              "      <th>rs4961</th>\n",
              "      <th>rs4994</th>\n",
              "      <th>rs17249754</th>\n",
              "      <th>rs2681472</th>\n",
              "      <th>rs1800247</th>\n",
              "      <th>rs3781719</th>\n",
              "      <th>rs1799998</th>\n",
              "      <th>rs11191548</th>\n",
              "      <th>rs9266359</th>\n",
              "      <th>...</th>\n",
              "      <th>rs1800206</th>\n",
              "      <th>rs4253778</th>\n",
              "      <th>rs11099098</th>\n",
              "      <th>rs2296545</th>\n",
              "      <th>rs1122608</th>\n",
              "      <th>rs6749447</th>\n",
              "      <th>rs2361159</th>\n",
              "      <th>rs2021783</th>\n",
              "      <th>rs6013382</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06ef20fc-9464-4cec-ab80-1bffd0023060')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-06ef20fc-9464-4cec-ab80-1bffd0023060 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-06ef20fc-9464-4cec-ab80-1bffd0023060');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b7bfef9f-6ffe-43bd-865f-42fb55105a84\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7bfef9f-6ffe-43bd-865f-42fb55105a84')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b7bfef9f-6ffe-43bd-865f-42fb55105a84 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RETF1Q2U2psG"
      },
      "source": [
        "Membagi data menjadi train dan test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "EtZSUuWF31S_"
      },
      "outputs": [],
      "source": [
        "y = df.pop('label')\n",
        "X = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Dbu-FWuB3vRE"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "svl_fxWQz9nW"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(X_train).to_excel('X_train.xlsx')\n",
        "pd.DataFrame(y_train).to_excel('y_train.xlsx')\n",
        "pd.DataFrame(X_test).to_excel('X_test.xlsx')\n",
        "pd.DataFrame(y_test).to_excel('y_test.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Existing Data"
      ],
      "metadata": {
        "id": "NPBhfSq5na88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import random\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# import tensorflow as tf\n",
        "# from keras.models import Sequential, load_model\n",
        "# from keras.layers import Dense, Dropout\n",
        "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from tensorflow.keras.layers import LSTM\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# import joblib\n",
        "# from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "# from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "# from matplotlib import pyplot\n",
        "# import time\n",
        "# import io\n",
        "\n",
        "# from google.colab import files\n",
        "\n",
        "# X_train = pd.read_excel('X_train.xlsx').drop(columns=['Unnamed: 0'])\n",
        "# X_test = pd.read_excel('X_test.xlsx').drop(columns=['Unnamed: 0'])\n",
        "# y_train = pd.read_excel('y_train.xlsx').drop(columns=['Unnamed: 0'])\n",
        "# y_test = pd.read_excel('y_test.xlsx').drop(columns=['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "6VilW8Dkngeu"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1lf8zgI-GqP"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whq77VpcNOBO",
        "outputId": "74ed0ad1-507c-4472-e0f8-46b7e2d1364a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Time: 0.005562305450439453 seconds\n",
            "Test Time: 0.11616349220275879 seconds\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "KNN = KNeighborsClassifier()\n",
        "KNN.fit(X_train, y_train)\n",
        "joblib.dump(KNN, \"knn.joblib\")\n",
        "\n",
        "print(\"Train Time: %s seconds\" % (time.time() - start_time))\n",
        "start_time = time.time()\n",
        "\n",
        "# KNN = joblib.load(\"knn.joblib\")\n",
        "knn_y_pred = KNN.predict(X_test)\n",
        "\n",
        "print(\"Test Time: %s seconds\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "bGW6zcMlNnKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72fe8506-5aef-499b-c101-f58ee42a2334"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'algorithm': 'auto',\n",
              " 'leaf_size': 30,\n",
              " 'metric': 'minkowski',\n",
              " 'metric_params': None,\n",
              " 'n_jobs': None,\n",
              " 'n_neighbors': 5,\n",
              " 'p': 2,\n",
              " 'weights': 'uniform'}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "KNN.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "PdJYZCxnNwE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd9976b-b08e-42b1-cad7-6feb600ee3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 6 10 15\n"
          ]
        }
      ],
      "source": [
        "if(len(confusion_matrix(y_test, knn_y_pred).ravel()) > 1):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, knn_y_pred).ravel()\n",
        "  print(tn, fp, fn, tp)\n",
        "else:\n",
        "  knn_cm = confusion_matrix(y_test, knn_y_pred)\n",
        "  print(knn_cm)\n",
        "knn_ac = accuracy_score(y_test, knn_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "S2EXejHIN2mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894bb82e-fead-4696-b168-29b85e9a3c14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5555555555555556"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "knn_ac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyhwVVrsTvsr"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "3cSVaft9T25A"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mzHFZbv0UJaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a719a9ac-2e4e-442f-ce3c-aed673d013d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Time: 0.01158761978149414 seconds\n",
            "Test Time: 0.005254983901977539 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "SVM = SVC(gamma='auto')\n",
        "SVM.fit(X_train, y_train)\n",
        "joblib.dump(SVM, \"svm.joblib\")\n",
        "\n",
        "print(\"Train Time: %s seconds\" % (time.time() - start_time))\n",
        "start_time = time.time()\n",
        "\n",
        "# SVM = joblib.load(\"svm.joblib\")\n",
        "svm_y_pred = SVM.predict(X_test)\n",
        "\n",
        "print(\"Test Time: %s seconds\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "G4hOhfQYU8Qd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89342dde-f274-4a32-8139-f6e516efcbfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'C': 1.0,\n",
              " 'break_ties': False,\n",
              " 'cache_size': 200,\n",
              " 'class_weight': None,\n",
              " 'coef0': 0.0,\n",
              " 'decision_function_shape': 'ovr',\n",
              " 'degree': 3,\n",
              " 'gamma': 'auto',\n",
              " 'kernel': 'rbf',\n",
              " 'max_iter': -1,\n",
              " 'probability': False,\n",
              " 'random_state': None,\n",
              " 'shrinking': True,\n",
              " 'tol': 0.001,\n",
              " 'verbose': False}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "SVM.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_8taJ_xOUryf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be758572-91e3-4c79-aaf5-b13d89d9e878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 7 4 21\n"
          ]
        }
      ],
      "source": [
        "if(len(confusion_matrix(y_test, svm_y_pred).ravel()) > 1):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, svm_y_pred).ravel()\n",
        "  print(tn, fp, fn, tp)\n",
        "else:\n",
        "  svm_cm = confusion_matrix(y_test, svm_y_pred)\n",
        "  print(svm_cm)\n",
        "svm_ac = accuracy_score(y_test,svm_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "J6O1cRkiU09Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca64e335-d904-447d-a542-bd538fa75f02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6944444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "svm_ac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqxu29MsTw-A"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Swdo4mFPT36B"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "9f87p3skVQ59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed188a7-d232-4ac8-98b8-e3631964d327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Time: 0.015041828155517578 seconds\n",
            "Test Time: 0.0028307437896728516 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "NB = MultinomialNB()\n",
        "NB.fit(X_train, y_train)\n",
        "joblib.dump(NB, \"nb.joblib\")\n",
        "\n",
        "print(\"Train Time: %s seconds\" % (time.time() - start_time))\n",
        "start_time = time.time()\n",
        "\n",
        "# NB = joblib.load(\"nb.joblib\")\n",
        "nb_y_pred = NB.predict(X_test)\n",
        "\n",
        "print(\"Test Time: %s seconds\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lwDSJNv8VY_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dcda0d9-2c68-4d8f-aead-a5afb67ebc24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': 'warn'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "NB.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "oVawt8ozVjt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d581b4f1-d7a1-4bd7-95f3-8540626dc599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 5 8 17\n"
          ]
        }
      ],
      "source": [
        "if(len(confusion_matrix(y_test, nb_y_pred).ravel()) > 1):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, nb_y_pred).ravel()\n",
        "  print(tn, fp, fn, tp)\n",
        "else:\n",
        "  nb_cm = confusion_matrix(y_test, nb_y_pred)\n",
        "  print(nb_cm)\n",
        "nb_ac = accuracy_score(y_test,nb_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Z8VegB5XVrg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "623c2433-fadd-40dc-c110-ac1abc11c17a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6388888888888888"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "nb_ac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxsSkq96Tzl9"
      },
      "source": [
        "# Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "nMZl5xQeT4xp"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fLngXsf7V2FS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17be6261-c37f-43df-e831-c581bdcfa9c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Time: 0.013299703598022461 seconds\n",
            "Test Time: 0.004426002502441406 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "DT = tree.DecisionTreeClassifier()\n",
        "DT.fit(X_train, y_train)\n",
        "joblib.dump(DT, \"dt.joblib\")\n",
        "\n",
        "print(\"Train Time: %s seconds\" % (time.time() - start_time))\n",
        "start_time = time.time()\n",
        "\n",
        "# DT = joblib.load(\"dt.joblib\")\n",
        "dt_y_pred = DT.predict(X_test)\n",
        "\n",
        "print(\"Test Time: %s seconds\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HAVGkmliV8lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a347e307-d170-4179-e972-e10ab0856582"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ccp_alpha': 0.0,\n",
              " 'class_weight': None,\n",
              " 'criterion': 'gini',\n",
              " 'max_depth': None,\n",
              " 'max_features': None,\n",
              " 'max_leaf_nodes': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'random_state': None,\n",
              " 'splitter': 'best'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "DT.get_params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "xtpK3q2MWBx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd6ec78-a7ac-4c9f-ca4e-fea0e8307db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 3 9 16\n"
          ]
        }
      ],
      "source": [
        "if(len(confusion_matrix(y_test, dt_y_pred).ravel()) > 1):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, dt_y_pred).ravel()\n",
        "  print(tn, fp, fn, tp)\n",
        "else:\n",
        "  dt_cm = confusion_matrix(y_test, dt_y_pred)\n",
        "  print(dt_cm)\n",
        "dt_ac = accuracy_score(y_test,dt_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "UJDd9TftWHL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1b5740-1391-4d62-e8aa-6f9cfc532f7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "dt_ac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aCVojWHT1Bz"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "D0eDiR3CT63U"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "R4zWVoVHWTV7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e543fc2c-990f-410e-e264-32364f677c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Time: 0.3329136371612549 seconds\n",
            "Test Time: 0.009665250778198242 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "RF = RandomForestClassifier()\n",
        "RF.fit(X_train, y_train)\n",
        "joblib.dump(RF, \"rf.joblib\")\n",
        "\n",
        "print(\"Train Time: %s seconds\" % (time.time() - start_time))\n",
        "start_time = time.time()\n",
        "\n",
        "# RF = joblib.load(\"rf.joblib\")\n",
        "rf_y_pred = RF.predict(X_test)\n",
        "\n",
        "print(\"Test Time: %s seconds\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF.get_params()"
      ],
      "metadata": {
        "id": "ENg8kkYwQ-2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ecf5103-a471-44a3-816e-0ae4fe499a1c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'ccp_alpha': 0.0,\n",
              " 'class_weight': None,\n",
              " 'criterion': 'gini',\n",
              " 'max_depth': None,\n",
              " 'max_features': 'sqrt',\n",
              " 'max_leaf_nodes': None,\n",
              " 'max_samples': None,\n",
              " 'min_impurity_decrease': 0.0,\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'min_weight_fraction_leaf': 0.0,\n",
              " 'n_estimators': 100,\n",
              " 'n_jobs': None,\n",
              " 'oob_score': False,\n",
              " 'random_state': None,\n",
              " 'verbose': 0,\n",
              " 'warm_start': False}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "wvKRl3R6Womm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "638905fb-6efe-4de5-f1c6-195dee7c6048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 4 5 20\n"
          ]
        }
      ],
      "source": [
        "if(len(confusion_matrix(y_test, rf_y_pred).ravel()) > 1):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, rf_y_pred).ravel()\n",
        "  print(tn, fp, fn, tp)\n",
        "else:\n",
        "  rf_cm = confusion_matrix(y_test, rf_y_pred)\n",
        "  print(rf_cm)\n",
        "rf_ac = accuracy_score(y_test, rf_y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ykMO7Xl-Wuqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f314e1-8042-41fb-fcab-32f37e2cdd65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "rf_ac"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVxzjx7r4Whs"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "s6Et3wbKCkb1"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.to_numpy().reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = X_test.to_numpy().reshape((X_test.shape[0], 1, X_test.shape[1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.asarray(X_train).astype('float32')\n",
        "X_test = np.asarray(X_test).astype('float32')"
      ],
      "metadata": {
        "id": "xBPvSyLyodkx"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YJ7plFhorl4",
        "outputId": "3b31f1e0-3810-4710-c71a-b2754fb86db4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(106, 1, 27)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ontSHruu4r9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba6f591-d67d-44df-c673-17054a170e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5000\n",
            "1/4 [======>.......................] - ETA: 17s - loss: 0.6932 - binary_accuracy: 0.5312\n",
            "Epoch 1: val_binary_accuracy improved from -inf to 0.69444, saving model to best_model.h5\n",
            "4/4 [==============================] - 7s 464ms/step - loss: 0.6923 - binary_accuracy: 0.5755 - val_loss: 0.6896 - val_binary_accuracy: 0.6944\n",
            "Epoch 2/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6927 - binary_accuracy: 0.5312\n",
            "Epoch 2: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6907 - binary_accuracy: 0.6132 - val_loss: 0.6876 - val_binary_accuracy: 0.6944\n",
            "Epoch 3/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6900 - binary_accuracy: 0.6250\n",
            "Epoch 3: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6888 - binary_accuracy: 0.6132 - val_loss: 0.6858 - val_binary_accuracy: 0.6944\n",
            "Epoch 4/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6942 - binary_accuracy: 0.4375"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.6877 - binary_accuracy: 0.6132 - val_loss: 0.6840 - val_binary_accuracy: 0.6944\n",
            "Epoch 5/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6912 - binary_accuracy: 0.5625\n",
            "Epoch 5: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6866 - binary_accuracy: 0.6132 - val_loss: 0.6821 - val_binary_accuracy: 0.6944\n",
            "Epoch 6/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6840 - binary_accuracy: 0.6562\n",
            "Epoch 6: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6869 - binary_accuracy: 0.6132 - val_loss: 0.6801 - val_binary_accuracy: 0.6944\n",
            "Epoch 7/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6853 - binary_accuracy: 0.5938\n",
            "Epoch 7: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.6834 - binary_accuracy: 0.6132 - val_loss: 0.6781 - val_binary_accuracy: 0.6944\n",
            "Epoch 8/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6738 - binary_accuracy: 0.7500\n",
            "Epoch 8: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.6838 - binary_accuracy: 0.6132 - val_loss: 0.6762 - val_binary_accuracy: 0.6944\n",
            "Epoch 9/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6841 - binary_accuracy: 0.5938\n",
            "Epoch 9: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.6816 - binary_accuracy: 0.6132 - val_loss: 0.6741 - val_binary_accuracy: 0.6944\n",
            "Epoch 10/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6867 - binary_accuracy: 0.5625\n",
            "Epoch 10: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6804 - binary_accuracy: 0.6132 - val_loss: 0.6722 - val_binary_accuracy: 0.6944\n",
            "Epoch 11/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6723 - binary_accuracy: 0.7188\n",
            "Epoch 11: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6807 - binary_accuracy: 0.6132 - val_loss: 0.6701 - val_binary_accuracy: 0.6944\n",
            "Epoch 12/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6640 - binary_accuracy: 0.7500\n",
            "Epoch 12: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6791 - binary_accuracy: 0.6132 - val_loss: 0.6684 - val_binary_accuracy: 0.6944\n",
            "Epoch 13/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6710 - binary_accuracy: 0.6562\n",
            "Epoch 13: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6778 - binary_accuracy: 0.6132 - val_loss: 0.6666 - val_binary_accuracy: 0.6944\n",
            "Epoch 14/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6859 - binary_accuracy: 0.5625\n",
            "Epoch 14: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.6776 - binary_accuracy: 0.6132 - val_loss: 0.6647 - val_binary_accuracy: 0.6944\n",
            "Epoch 15/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6722 - binary_accuracy: 0.6250\n",
            "Epoch 15: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6745 - binary_accuracy: 0.6132 - val_loss: 0.6626 - val_binary_accuracy: 0.6944\n",
            "Epoch 16/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6819 - binary_accuracy: 0.5625\n",
            "Epoch 16: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.6768 - binary_accuracy: 0.6132 - val_loss: 0.6606 - val_binary_accuracy: 0.6944\n",
            "Epoch 17/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6666 - binary_accuracy: 0.6875\n",
            "Epoch 17: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6756 - binary_accuracy: 0.6132 - val_loss: 0.6584 - val_binary_accuracy: 0.6944\n",
            "Epoch 18/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6922 - binary_accuracy: 0.5000\n",
            "Epoch 18: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6743 - binary_accuracy: 0.6132 - val_loss: 0.6566 - val_binary_accuracy: 0.6944\n",
            "Epoch 19/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6674 - binary_accuracy: 0.5938\n",
            "Epoch 19: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.6707 - binary_accuracy: 0.6132 - val_loss: 0.6553 - val_binary_accuracy: 0.6944\n",
            "Epoch 20/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6430 - binary_accuracy: 0.7500\n",
            "Epoch 20: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6730 - binary_accuracy: 0.6132 - val_loss: 0.6538 - val_binary_accuracy: 0.6944\n",
            "Epoch 21/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6935 - binary_accuracy: 0.5312\n",
            "Epoch 21: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6708 - binary_accuracy: 0.6132 - val_loss: 0.6526 - val_binary_accuracy: 0.6944\n",
            "Epoch 22/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6516 - binary_accuracy: 0.7188\n",
            "Epoch 22: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6724 - binary_accuracy: 0.6132 - val_loss: 0.6508 - val_binary_accuracy: 0.6944\n",
            "Epoch 23/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6900 - binary_accuracy: 0.5312\n",
            "Epoch 23: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6663 - binary_accuracy: 0.6132 - val_loss: 0.6489 - val_binary_accuracy: 0.6944\n",
            "Epoch 24/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6731 - binary_accuracy: 0.5938\n",
            "Epoch 24: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6659 - binary_accuracy: 0.6132 - val_loss: 0.6471 - val_binary_accuracy: 0.6944\n",
            "Epoch 25/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6846 - binary_accuracy: 0.5625\n",
            "Epoch 25: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6676 - binary_accuracy: 0.6132 - val_loss: 0.6456 - val_binary_accuracy: 0.6944\n",
            "Epoch 26/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6841 - binary_accuracy: 0.5625\n",
            "Epoch 26: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6654 - binary_accuracy: 0.6132 - val_loss: 0.6440 - val_binary_accuracy: 0.6944\n",
            "Epoch 27/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6396 - binary_accuracy: 0.6875\n",
            "Epoch 27: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6653 - binary_accuracy: 0.6132 - val_loss: 0.6424 - val_binary_accuracy: 0.6944\n",
            "Epoch 28/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6569 - binary_accuracy: 0.6562\n",
            "Epoch 28: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6626 - binary_accuracy: 0.6132 - val_loss: 0.6412 - val_binary_accuracy: 0.6944\n",
            "Epoch 29/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6672 - binary_accuracy: 0.6250\n",
            "Epoch 29: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6674 - binary_accuracy: 0.6132 - val_loss: 0.6404 - val_binary_accuracy: 0.6944\n",
            "Epoch 30/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6754 - binary_accuracy: 0.5938\n",
            "Epoch 30: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.6649 - binary_accuracy: 0.6132 - val_loss: 0.6392 - val_binary_accuracy: 0.6944\n",
            "Epoch 31/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6810 - binary_accuracy: 0.5938\n",
            "Epoch 31: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6640 - binary_accuracy: 0.6132 - val_loss: 0.6382 - val_binary_accuracy: 0.6944\n",
            "Epoch 32/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6261 - binary_accuracy: 0.7188\n",
            "Epoch 32: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6686 - binary_accuracy: 0.6132 - val_loss: 0.6370 - val_binary_accuracy: 0.6944\n",
            "Epoch 33/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6476 - binary_accuracy: 0.6875\n",
            "Epoch 33: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.6662 - binary_accuracy: 0.6132 - val_loss: 0.6357 - val_binary_accuracy: 0.6944\n",
            "Epoch 34/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6709 - binary_accuracy: 0.5938\n",
            "Epoch 34: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6635 - binary_accuracy: 0.6132 - val_loss: 0.6349 - val_binary_accuracy: 0.6944\n",
            "Epoch 35/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6525 - binary_accuracy: 0.6250\n",
            "Epoch 35: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6587 - binary_accuracy: 0.6132 - val_loss: 0.6341 - val_binary_accuracy: 0.6944\n",
            "Epoch 36/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6258 - binary_accuracy: 0.7188\n",
            "Epoch 36: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6614 - binary_accuracy: 0.6132 - val_loss: 0.6333 - val_binary_accuracy: 0.6944\n",
            "Epoch 37/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6782 - binary_accuracy: 0.5625\n",
            "Epoch 37: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.6640 - binary_accuracy: 0.6132 - val_loss: 0.6327 - val_binary_accuracy: 0.6944\n",
            "Epoch 38/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6238 - binary_accuracy: 0.6875\n",
            "Epoch 38: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.6563 - binary_accuracy: 0.6132 - val_loss: 0.6319 - val_binary_accuracy: 0.6944\n",
            "Epoch 39/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6806 - binary_accuracy: 0.5938\n",
            "Epoch 39: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6606 - binary_accuracy: 0.6132 - val_loss: 0.6314 - val_binary_accuracy: 0.6944\n",
            "Epoch 40/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6297 - binary_accuracy: 0.6562\n",
            "Epoch 40: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6616 - binary_accuracy: 0.6132 - val_loss: 0.6309 - val_binary_accuracy: 0.6944\n",
            "Epoch 41/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6618 - binary_accuracy: 0.5938\n",
            "Epoch 41: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6524 - binary_accuracy: 0.6132 - val_loss: 0.6305 - val_binary_accuracy: 0.6944\n",
            "Epoch 42/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6531 - binary_accuracy: 0.5625\n",
            "Epoch 42: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6493 - binary_accuracy: 0.6132 - val_loss: 0.6294 - val_binary_accuracy: 0.6944\n",
            "Epoch 43/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6323 - binary_accuracy: 0.6875\n",
            "Epoch 43: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6523 - binary_accuracy: 0.6132 - val_loss: 0.6280 - val_binary_accuracy: 0.6944\n",
            "Epoch 44/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6370 - binary_accuracy: 0.6562\n",
            "Epoch 44: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6584 - binary_accuracy: 0.6132 - val_loss: 0.6267 - val_binary_accuracy: 0.6944\n",
            "Epoch 45/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6555 - binary_accuracy: 0.6562\n",
            "Epoch 45: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.6616 - binary_accuracy: 0.6132 - val_loss: 0.6259 - val_binary_accuracy: 0.6944\n",
            "Epoch 46/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6617 - binary_accuracy: 0.5625\n",
            "Epoch 46: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.6479 - binary_accuracy: 0.6132 - val_loss: 0.6250 - val_binary_accuracy: 0.6944\n",
            "Epoch 47/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6626 - binary_accuracy: 0.5938\n",
            "Epoch 47: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6544 - binary_accuracy: 0.6132 - val_loss: 0.6241 - val_binary_accuracy: 0.6944\n",
            "Epoch 48/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6294 - binary_accuracy: 0.6562\n",
            "Epoch 48: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6561 - binary_accuracy: 0.6132 - val_loss: 0.6236 - val_binary_accuracy: 0.6944\n",
            "Epoch 49/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6175 - binary_accuracy: 0.6562\n",
            "Epoch 49: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6488 - binary_accuracy: 0.6132 - val_loss: 0.6235 - val_binary_accuracy: 0.6944\n",
            "Epoch 50/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6414 - binary_accuracy: 0.6875\n",
            "Epoch 50: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.6563 - binary_accuracy: 0.6132 - val_loss: 0.6230 - val_binary_accuracy: 0.6944\n",
            "Epoch 51/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6263 - binary_accuracy: 0.7188\n",
            "Epoch 51: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6522 - binary_accuracy: 0.6132 - val_loss: 0.6225 - val_binary_accuracy: 0.6944\n",
            "Epoch 52/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7402 - binary_accuracy: 0.4375\n",
            "Epoch 52: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6547 - binary_accuracy: 0.6132 - val_loss: 0.6227 - val_binary_accuracy: 0.6944\n",
            "Epoch 53/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6403 - binary_accuracy: 0.5938\n",
            "Epoch 53: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6505 - binary_accuracy: 0.6132 - val_loss: 0.6224 - val_binary_accuracy: 0.6944\n",
            "Epoch 54/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6734 - binary_accuracy: 0.5938\n",
            "Epoch 54: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6476 - binary_accuracy: 0.6132 - val_loss: 0.6217 - val_binary_accuracy: 0.6944\n",
            "Epoch 55/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6285 - binary_accuracy: 0.6562\n",
            "Epoch 55: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6486 - binary_accuracy: 0.6132 - val_loss: 0.6204 - val_binary_accuracy: 0.6944\n",
            "Epoch 56/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6388 - binary_accuracy: 0.5938\n",
            "Epoch 56: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6416 - binary_accuracy: 0.6132 - val_loss: 0.6186 - val_binary_accuracy: 0.6944\n",
            "Epoch 57/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6175 - binary_accuracy: 0.6875\n",
            "Epoch 57: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6456 - binary_accuracy: 0.6132 - val_loss: 0.6167 - val_binary_accuracy: 0.6944\n",
            "Epoch 58/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6864 - binary_accuracy: 0.5312\n",
            "Epoch 58: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6480 - binary_accuracy: 0.6132 - val_loss: 0.6151 - val_binary_accuracy: 0.6944\n",
            "Epoch 59/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6404 - binary_accuracy: 0.5938\n",
            "Epoch 59: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6477 - binary_accuracy: 0.6132 - val_loss: 0.6138 - val_binary_accuracy: 0.6944\n",
            "Epoch 60/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6212 - binary_accuracy: 0.6562\n",
            "Epoch 60: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6478 - binary_accuracy: 0.6132 - val_loss: 0.6125 - val_binary_accuracy: 0.6944\n",
            "Epoch 61/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6298 - binary_accuracy: 0.6250\n",
            "Epoch 61: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.6438 - binary_accuracy: 0.6132 - val_loss: 0.6117 - val_binary_accuracy: 0.6944\n",
            "Epoch 62/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6772 - binary_accuracy: 0.5938\n",
            "Epoch 62: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.6496 - binary_accuracy: 0.6132 - val_loss: 0.6116 - val_binary_accuracy: 0.6944\n",
            "Epoch 63/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6884 - binary_accuracy: 0.5312\n",
            "Epoch 63: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6428 - binary_accuracy: 0.6132 - val_loss: 0.6124 - val_binary_accuracy: 0.6944\n",
            "Epoch 64/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6614 - binary_accuracy: 0.5625\n",
            "Epoch 64: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6364 - binary_accuracy: 0.6132 - val_loss: 0.6123 - val_binary_accuracy: 0.6944\n",
            "Epoch 65/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6360 - binary_accuracy: 0.6250\n",
            "Epoch 65: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.6445 - binary_accuracy: 0.6132 - val_loss: 0.6120 - val_binary_accuracy: 0.6944\n",
            "Epoch 66/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6141 - binary_accuracy: 0.6250\n",
            "Epoch 66: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.6260 - binary_accuracy: 0.6132 - val_loss: 0.6118 - val_binary_accuracy: 0.6944\n",
            "Epoch 67/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6667 - binary_accuracy: 0.5312\n",
            "Epoch 67: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.6419 - binary_accuracy: 0.6132 - val_loss: 0.6118 - val_binary_accuracy: 0.6944\n",
            "Epoch 68/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5957 - binary_accuracy: 0.6875\n",
            "Epoch 68: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6364 - binary_accuracy: 0.6132 - val_loss: 0.6117 - val_binary_accuracy: 0.6944\n",
            "Epoch 69/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5747 - binary_accuracy: 0.7500\n",
            "Epoch 69: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6428 - binary_accuracy: 0.6132 - val_loss: 0.6119 - val_binary_accuracy: 0.6944\n",
            "Epoch 70/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.7017 - binary_accuracy: 0.4688\n",
            "Epoch 70: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.6366 - binary_accuracy: 0.6226 - val_loss: 0.6128 - val_binary_accuracy: 0.6944\n",
            "Epoch 71/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6234 - binary_accuracy: 0.6875\n",
            "Epoch 71: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.6387 - binary_accuracy: 0.6132 - val_loss: 0.6126 - val_binary_accuracy: 0.6944\n",
            "Epoch 72/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6101 - binary_accuracy: 0.6875\n",
            "Epoch 72: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6409 - binary_accuracy: 0.6132 - val_loss: 0.6128 - val_binary_accuracy: 0.6944\n",
            "Epoch 73/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6365 - binary_accuracy: 0.5625\n",
            "Epoch 73: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.6353 - binary_accuracy: 0.6132 - val_loss: 0.6126 - val_binary_accuracy: 0.6944\n",
            "Epoch 74/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5891 - binary_accuracy: 0.6562\n",
            "Epoch 74: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6255 - binary_accuracy: 0.6226 - val_loss: 0.6112 - val_binary_accuracy: 0.6944\n",
            "Epoch 75/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5916 - binary_accuracy: 0.6875\n",
            "Epoch 75: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.6293 - binary_accuracy: 0.6226 - val_loss: 0.6099 - val_binary_accuracy: 0.6944\n",
            "Epoch 76/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6438 - binary_accuracy: 0.5938\n",
            "Epoch 76: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.6388 - binary_accuracy: 0.6132 - val_loss: 0.6095 - val_binary_accuracy: 0.6944\n",
            "Epoch 77/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6911 - binary_accuracy: 0.5312\n",
            "Epoch 77: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.6283 - binary_accuracy: 0.6226 - val_loss: 0.6085 - val_binary_accuracy: 0.6944\n",
            "Epoch 78/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6360 - binary_accuracy: 0.5625\n",
            "Epoch 78: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6296 - binary_accuracy: 0.6226 - val_loss: 0.6069 - val_binary_accuracy: 0.6944\n",
            "Epoch 79/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6324 - binary_accuracy: 0.5938\n",
            "Epoch 79: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6255 - binary_accuracy: 0.6321 - val_loss: 0.6051 - val_binary_accuracy: 0.6944\n",
            "Epoch 80/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5566 - binary_accuracy: 0.7500\n",
            "Epoch 80: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.6192 - binary_accuracy: 0.6226 - val_loss: 0.6034 - val_binary_accuracy: 0.6944\n",
            "Epoch 81/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5755 - binary_accuracy: 0.7500\n",
            "Epoch 81: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6261 - binary_accuracy: 0.6415 - val_loss: 0.6017 - val_binary_accuracy: 0.6944\n",
            "Epoch 82/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6337 - binary_accuracy: 0.6250\n",
            "Epoch 82: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6227 - binary_accuracy: 0.6226 - val_loss: 0.6007 - val_binary_accuracy: 0.6944\n",
            "Epoch 83/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5956 - binary_accuracy: 0.6250\n",
            "Epoch 83: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6165 - binary_accuracy: 0.6132 - val_loss: 0.6004 - val_binary_accuracy: 0.6944\n",
            "Epoch 84/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5925 - binary_accuracy: 0.6250\n",
            "Epoch 84: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6255 - binary_accuracy: 0.6226 - val_loss: 0.6004 - val_binary_accuracy: 0.6944\n",
            "Epoch 85/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6232 - binary_accuracy: 0.6562\n",
            "Epoch 85: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.6341 - binary_accuracy: 0.6226 - val_loss: 0.6002 - val_binary_accuracy: 0.6944\n",
            "Epoch 86/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6607 - binary_accuracy: 0.5625\n",
            "Epoch 86: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6198 - binary_accuracy: 0.6226 - val_loss: 0.5991 - val_binary_accuracy: 0.6944\n",
            "Epoch 87/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6095 - binary_accuracy: 0.6250\n",
            "Epoch 87: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6193 - binary_accuracy: 0.6321 - val_loss: 0.5976 - val_binary_accuracy: 0.6944\n",
            "Epoch 88/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5984 - binary_accuracy: 0.6562\n",
            "Epoch 88: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6204 - binary_accuracy: 0.6226 - val_loss: 0.5958 - val_binary_accuracy: 0.6944\n",
            "Epoch 89/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6313 - binary_accuracy: 0.6875\n",
            "Epoch 89: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6226 - binary_accuracy: 0.6415 - val_loss: 0.5946 - val_binary_accuracy: 0.6944\n",
            "Epoch 90/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5747 - binary_accuracy: 0.7188\n",
            "Epoch 90: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6171 - binary_accuracy: 0.6509 - val_loss: 0.5942 - val_binary_accuracy: 0.6944\n",
            "Epoch 91/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5959 - binary_accuracy: 0.6250\n",
            "Epoch 91: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.6108 - binary_accuracy: 0.6226 - val_loss: 0.5945 - val_binary_accuracy: 0.6944\n",
            "Epoch 92/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6255 - binary_accuracy: 0.5938\n",
            "Epoch 92: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6051 - binary_accuracy: 0.6321 - val_loss: 0.5949 - val_binary_accuracy: 0.6944\n",
            "Epoch 93/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5514 - binary_accuracy: 0.7188\n",
            "Epoch 93: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6019 - binary_accuracy: 0.6509 - val_loss: 0.5945 - val_binary_accuracy: 0.6944\n",
            "Epoch 94/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5643 - binary_accuracy: 0.7188\n",
            "Epoch 94: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.5918 - binary_accuracy: 0.6698 - val_loss: 0.5930 - val_binary_accuracy: 0.6944\n",
            "Epoch 95/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6291 - binary_accuracy: 0.6250\n",
            "Epoch 95: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.6020 - binary_accuracy: 0.6415 - val_loss: 0.5911 - val_binary_accuracy: 0.6944\n",
            "Epoch 96/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5707 - binary_accuracy: 0.6875\n",
            "Epoch 96: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.6082 - binary_accuracy: 0.6509 - val_loss: 0.5893 - val_binary_accuracy: 0.6944\n",
            "Epoch 97/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4619 - binary_accuracy: 0.8438\n",
            "Epoch 97: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6020 - binary_accuracy: 0.6509 - val_loss: 0.5877 - val_binary_accuracy: 0.6944\n",
            "Epoch 98/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6304 - binary_accuracy: 0.6250\n",
            "Epoch 98: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6064 - binary_accuracy: 0.6415 - val_loss: 0.5882 - val_binary_accuracy: 0.6667\n",
            "Epoch 99/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6086 - binary_accuracy: 0.6562\n",
            "Epoch 99: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5994 - binary_accuracy: 0.6415 - val_loss: 0.5887 - val_binary_accuracy: 0.6667\n",
            "Epoch 100/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6738 - binary_accuracy: 0.5625\n",
            "Epoch 100: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.6097 - binary_accuracy: 0.6604 - val_loss: 0.5883 - val_binary_accuracy: 0.6667\n",
            "Epoch 101/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5781 - binary_accuracy: 0.6250\n",
            "Epoch 101: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.5893 - binary_accuracy: 0.6604 - val_loss: 0.5876 - val_binary_accuracy: 0.6667\n",
            "Epoch 102/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5489 - binary_accuracy: 0.6875\n",
            "Epoch 102: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.5934 - binary_accuracy: 0.6698 - val_loss: 0.5870 - val_binary_accuracy: 0.6667\n",
            "Epoch 103/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6141 - binary_accuracy: 0.5625\n",
            "Epoch 103: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5935 - binary_accuracy: 0.6415 - val_loss: 0.5869 - val_binary_accuracy: 0.6667\n",
            "Epoch 104/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5402 - binary_accuracy: 0.7188\n",
            "Epoch 104: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5807 - binary_accuracy: 0.6698 - val_loss: 0.5869 - val_binary_accuracy: 0.6944\n",
            "Epoch 105/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5790 - binary_accuracy: 0.6250\n",
            "Epoch 105: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.5874 - binary_accuracy: 0.6604 - val_loss: 0.5859 - val_binary_accuracy: 0.6944\n",
            "Epoch 106/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6901 - binary_accuracy: 0.5312\n",
            "Epoch 106: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5983 - binary_accuracy: 0.6509 - val_loss: 0.5858 - val_binary_accuracy: 0.6944\n",
            "Epoch 107/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5994 - binary_accuracy: 0.6562\n",
            "Epoch 107: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.6054 - binary_accuracy: 0.6321 - val_loss: 0.5855 - val_binary_accuracy: 0.6944\n",
            "Epoch 108/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5822 - binary_accuracy: 0.6562\n",
            "Epoch 108: val_binary_accuracy did not improve from 0.69444\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5676 - binary_accuracy: 0.6887 - val_loss: 0.5841 - val_binary_accuracy: 0.6944\n",
            "Epoch 109/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6098 - binary_accuracy: 0.6562\n",
            "Epoch 109: val_binary_accuracy improved from 0.69444 to 0.72222, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.5776 - binary_accuracy: 0.6981 - val_loss: 0.5835 - val_binary_accuracy: 0.7222\n",
            "Epoch 110/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5994 - binary_accuracy: 0.5312\n",
            "Epoch 110: val_binary_accuracy did not improve from 0.72222\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5675 - binary_accuracy: 0.6981 - val_loss: 0.5839 - val_binary_accuracy: 0.7222\n",
            "Epoch 111/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5557 - binary_accuracy: 0.6562\n",
            "Epoch 111: val_binary_accuracy did not improve from 0.72222\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5783 - binary_accuracy: 0.6604 - val_loss: 0.5848 - val_binary_accuracy: 0.7222\n",
            "Epoch 112/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5801 - binary_accuracy: 0.6250\n",
            "Epoch 112: val_binary_accuracy did not improve from 0.72222\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.5668 - binary_accuracy: 0.6698 - val_loss: 0.5854 - val_binary_accuracy: 0.7222\n",
            "Epoch 113/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5102 - binary_accuracy: 0.7812\n",
            "Epoch 113: val_binary_accuracy did not improve from 0.72222\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5670 - binary_accuracy: 0.6887 - val_loss: 0.5854 - val_binary_accuracy: 0.7222\n",
            "Epoch 114/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5721 - binary_accuracy: 0.6562\n",
            "Epoch 114: val_binary_accuracy improved from 0.72222 to 0.75000, saving model to best_model.h5\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.5658 - binary_accuracy: 0.7264 - val_loss: 0.5848 - val_binary_accuracy: 0.7500\n",
            "Epoch 115/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6016 - binary_accuracy: 0.7188\n",
            "Epoch 115: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.5711 - binary_accuracy: 0.6981 - val_loss: 0.5841 - val_binary_accuracy: 0.7500\n",
            "Epoch 116/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5904 - binary_accuracy: 0.6875\n",
            "Epoch 116: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5635 - binary_accuracy: 0.7075 - val_loss: 0.5836 - val_binary_accuracy: 0.7500\n",
            "Epoch 117/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5440 - binary_accuracy: 0.6250\n",
            "Epoch 117: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.5523 - binary_accuracy: 0.7170 - val_loss: 0.5830 - val_binary_accuracy: 0.7500\n",
            "Epoch 118/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5764 - binary_accuracy: 0.6250\n",
            "Epoch 118: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5607 - binary_accuracy: 0.7075 - val_loss: 0.5816 - val_binary_accuracy: 0.7500\n",
            "Epoch 119/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4978 - binary_accuracy: 0.7500\n",
            "Epoch 119: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5474 - binary_accuracy: 0.6981 - val_loss: 0.5808 - val_binary_accuracy: 0.7500\n",
            "Epoch 120/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5401 - binary_accuracy: 0.6875\n",
            "Epoch 120: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5468 - binary_accuracy: 0.6887 - val_loss: 0.5793 - val_binary_accuracy: 0.7500\n",
            "Epoch 121/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5860 - binary_accuracy: 0.6562\n",
            "Epoch 121: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5511 - binary_accuracy: 0.6981 - val_loss: 0.5772 - val_binary_accuracy: 0.7500\n",
            "Epoch 122/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5174 - binary_accuracy: 0.7812\n",
            "Epoch 122: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5255 - binary_accuracy: 0.7075 - val_loss: 0.5757 - val_binary_accuracy: 0.7500\n",
            "Epoch 123/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4758 - binary_accuracy: 0.7500\n",
            "Epoch 123: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.5419 - binary_accuracy: 0.7358 - val_loss: 0.5748 - val_binary_accuracy: 0.7500\n",
            "Epoch 124/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5434 - binary_accuracy: 0.7188\n",
            "Epoch 124: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5367 - binary_accuracy: 0.7264 - val_loss: 0.5737 - val_binary_accuracy: 0.7500\n",
            "Epoch 125/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4923 - binary_accuracy: 0.7500\n",
            "Epoch 125: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.5343 - binary_accuracy: 0.7170 - val_loss: 0.5746 - val_binary_accuracy: 0.7222\n",
            "Epoch 126/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5339 - binary_accuracy: 0.7500\n",
            "Epoch 126: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5435 - binary_accuracy: 0.7453 - val_loss: 0.5763 - val_binary_accuracy: 0.7500\n",
            "Epoch 127/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5326 - binary_accuracy: 0.7812\n",
            "Epoch 127: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.5399 - binary_accuracy: 0.7642 - val_loss: 0.5777 - val_binary_accuracy: 0.7500\n",
            "Epoch 128/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5932 - binary_accuracy: 0.6875\n",
            "Epoch 128: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5448 - binary_accuracy: 0.7264 - val_loss: 0.5772 - val_binary_accuracy: 0.7500\n",
            "Epoch 129/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4097 - binary_accuracy: 0.9062\n",
            "Epoch 129: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5292 - binary_accuracy: 0.7830 - val_loss: 0.5761 - val_binary_accuracy: 0.7500\n",
            "Epoch 130/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5822 - binary_accuracy: 0.5938\n",
            "Epoch 130: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5200 - binary_accuracy: 0.7358 - val_loss: 0.5791 - val_binary_accuracy: 0.7222\n",
            "Epoch 131/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5346 - binary_accuracy: 0.6250\n",
            "Epoch 131: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5265 - binary_accuracy: 0.7547 - val_loss: 0.5830 - val_binary_accuracy: 0.7222\n",
            "Epoch 132/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4526 - binary_accuracy: 0.9375\n",
            "Epoch 132: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5053 - binary_accuracy: 0.7642 - val_loss: 0.5821 - val_binary_accuracy: 0.7222\n",
            "Epoch 133/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5738 - binary_accuracy: 0.6562\n",
            "Epoch 133: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.5314 - binary_accuracy: 0.7547 - val_loss: 0.5829 - val_binary_accuracy: 0.7222\n",
            "Epoch 134/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5934 - binary_accuracy: 0.7500\n",
            "Epoch 134: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5181 - binary_accuracy: 0.7830 - val_loss: 0.5841 - val_binary_accuracy: 0.7222\n",
            "Epoch 135/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5507 - binary_accuracy: 0.7812\n",
            "Epoch 135: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.5247 - binary_accuracy: 0.7264 - val_loss: 0.5844 - val_binary_accuracy: 0.7222\n",
            "Epoch 136/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4859 - binary_accuracy: 0.7812\n",
            "Epoch 136: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.5110 - binary_accuracy: 0.7736 - val_loss: 0.5835 - val_binary_accuracy: 0.7222\n",
            "Epoch 137/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5039 - binary_accuracy: 0.7188\n",
            "Epoch 137: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4961 - binary_accuracy: 0.7264 - val_loss: 0.5840 - val_binary_accuracy: 0.7222\n",
            "Epoch 138/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5856 - binary_accuracy: 0.6250\n",
            "Epoch 138: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.5215 - binary_accuracy: 0.7453 - val_loss: 0.5851 - val_binary_accuracy: 0.7222\n",
            "Epoch 139/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4196 - binary_accuracy: 0.8750\n",
            "Epoch 139: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.5012 - binary_accuracy: 0.7925 - val_loss: 0.5830 - val_binary_accuracy: 0.7222\n",
            "Epoch 140/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4790 - binary_accuracy: 0.7188\n",
            "Epoch 140: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4722 - binary_accuracy: 0.7736 - val_loss: 0.5811 - val_binary_accuracy: 0.7222\n",
            "Epoch 141/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6740 - binary_accuracy: 0.6875\n",
            "Epoch 141: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5118 - binary_accuracy: 0.7736 - val_loss: 0.5798 - val_binary_accuracy: 0.7222\n",
            "Epoch 142/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5464 - binary_accuracy: 0.7500\n",
            "Epoch 142: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.5156 - binary_accuracy: 0.7736 - val_loss: 0.5793 - val_binary_accuracy: 0.7222\n",
            "Epoch 143/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4726 - binary_accuracy: 0.7812\n",
            "Epoch 143: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4803 - binary_accuracy: 0.7830 - val_loss: 0.5816 - val_binary_accuracy: 0.6944\n",
            "Epoch 144/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4616 - binary_accuracy: 0.8750\n",
            "Epoch 144: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.5170 - binary_accuracy: 0.8113 - val_loss: 0.5830 - val_binary_accuracy: 0.6944\n",
            "Epoch 145/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4296 - binary_accuracy: 0.8750\n",
            "Epoch 145: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.4720 - binary_accuracy: 0.8491 - val_loss: 0.5829 - val_binary_accuracy: 0.6944\n",
            "Epoch 146/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4619 - binary_accuracy: 0.7188\n",
            "Epoch 146: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4808 - binary_accuracy: 0.7642 - val_loss: 0.5817 - val_binary_accuracy: 0.6944\n",
            "Epoch 147/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3825 - binary_accuracy: 0.8438\n",
            "Epoch 147: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.4845 - binary_accuracy: 0.7830 - val_loss: 0.5793 - val_binary_accuracy: 0.6944\n",
            "Epoch 148/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5231 - binary_accuracy: 0.6875\n",
            "Epoch 148: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4830 - binary_accuracy: 0.7547 - val_loss: 0.5811 - val_binary_accuracy: 0.6944\n",
            "Epoch 149/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4351 - binary_accuracy: 0.7812\n",
            "Epoch 149: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4574 - binary_accuracy: 0.8302 - val_loss: 0.5837 - val_binary_accuracy: 0.6944\n",
            "Epoch 150/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4524 - binary_accuracy: 0.9062\n",
            "Epoch 150: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4866 - binary_accuracy: 0.8208 - val_loss: 0.5845 - val_binary_accuracy: 0.6667\n",
            "Epoch 151/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5812 - binary_accuracy: 0.7188\n",
            "Epoch 151: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.4663 - binary_accuracy: 0.8113 - val_loss: 0.5860 - val_binary_accuracy: 0.6667\n",
            "Epoch 152/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5340 - binary_accuracy: 0.7188\n",
            "Epoch 152: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4851 - binary_accuracy: 0.7925 - val_loss: 0.5888 - val_binary_accuracy: 0.6667\n",
            "Epoch 153/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4999 - binary_accuracy: 0.7812\n",
            "Epoch 153: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4643 - binary_accuracy: 0.7925 - val_loss: 0.5908 - val_binary_accuracy: 0.6667\n",
            "Epoch 154/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4934 - binary_accuracy: 0.7812\n",
            "Epoch 154: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4806 - binary_accuracy: 0.8019 - val_loss: 0.5926 - val_binary_accuracy: 0.6111\n",
            "Epoch 155/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4061 - binary_accuracy: 0.8438\n",
            "Epoch 155: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.4785 - binary_accuracy: 0.8113 - val_loss: 0.5917 - val_binary_accuracy: 0.6667\n",
            "Epoch 156/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4863 - binary_accuracy: 0.8750\n",
            "Epoch 156: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.4942 - binary_accuracy: 0.8019 - val_loss: 0.5917 - val_binary_accuracy: 0.6667\n",
            "Epoch 157/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5338 - binary_accuracy: 0.6875\n",
            "Epoch 157: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.4647 - binary_accuracy: 0.8019 - val_loss: 0.5948 - val_binary_accuracy: 0.6111\n",
            "Epoch 158/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5023 - binary_accuracy: 0.8438\n",
            "Epoch 158: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4446 - binary_accuracy: 0.8396 - val_loss: 0.5963 - val_binary_accuracy: 0.6111\n",
            "Epoch 159/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4514 - binary_accuracy: 0.8750\n",
            "Epoch 159: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4480 - binary_accuracy: 0.8019 - val_loss: 0.5957 - val_binary_accuracy: 0.6111\n",
            "Epoch 160/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4475 - binary_accuracy: 0.8750\n",
            "Epoch 160: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4618 - binary_accuracy: 0.8208 - val_loss: 0.5960 - val_binary_accuracy: 0.6667\n",
            "Epoch 161/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4163 - binary_accuracy: 0.8438\n",
            "Epoch 161: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.4302 - binary_accuracy: 0.8019 - val_loss: 0.5998 - val_binary_accuracy: 0.6111\n",
            "Epoch 162/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5868 - binary_accuracy: 0.7500\n",
            "Epoch 162: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4767 - binary_accuracy: 0.7925 - val_loss: 0.6040 - val_binary_accuracy: 0.6111\n",
            "Epoch 163/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5733 - binary_accuracy: 0.7500\n",
            "Epoch 163: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4681 - binary_accuracy: 0.8113 - val_loss: 0.6069 - val_binary_accuracy: 0.6111\n",
            "Epoch 164/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3798 - binary_accuracy: 0.8750\n",
            "Epoch 164: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4381 - binary_accuracy: 0.8396 - val_loss: 0.6048 - val_binary_accuracy: 0.6111\n",
            "Epoch 165/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4463 - binary_accuracy: 0.8125\n",
            "Epoch 165: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4520 - binary_accuracy: 0.8019 - val_loss: 0.6025 - val_binary_accuracy: 0.6111\n",
            "Epoch 166/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4266 - binary_accuracy: 0.9062\n",
            "Epoch 166: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4345 - binary_accuracy: 0.8585 - val_loss: 0.6028 - val_binary_accuracy: 0.6667\n",
            "Epoch 167/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4827 - binary_accuracy: 0.7812\n",
            "Epoch 167: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.4701 - binary_accuracy: 0.7925 - val_loss: 0.6049 - val_binary_accuracy: 0.6111\n",
            "Epoch 168/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4546 - binary_accuracy: 0.7812\n",
            "Epoch 168: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4342 - binary_accuracy: 0.8396 - val_loss: 0.6092 - val_binary_accuracy: 0.6111\n",
            "Epoch 169/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4752 - binary_accuracy: 0.7812\n",
            "Epoch 169: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.4367 - binary_accuracy: 0.8113 - val_loss: 0.6141 - val_binary_accuracy: 0.6111\n",
            "Epoch 170/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4615 - binary_accuracy: 0.8750\n",
            "Epoch 170: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.4397 - binary_accuracy: 0.8491 - val_loss: 0.6202 - val_binary_accuracy: 0.6111\n",
            "Epoch 171/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4180 - binary_accuracy: 0.8750\n",
            "Epoch 171: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.4177 - binary_accuracy: 0.8585 - val_loss: 0.6237 - val_binary_accuracy: 0.6111\n",
            "Epoch 172/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.6245 - binary_accuracy: 0.7188\n",
            "Epoch 172: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.4396 - binary_accuracy: 0.8396 - val_loss: 0.6309 - val_binary_accuracy: 0.6111\n",
            "Epoch 173/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3637 - binary_accuracy: 0.9375\n",
            "Epoch 173: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4159 - binary_accuracy: 0.8396 - val_loss: 0.6335 - val_binary_accuracy: 0.6111\n",
            "Epoch 174/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4278 - binary_accuracy: 0.8125\n",
            "Epoch 174: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.4385 - binary_accuracy: 0.8019 - val_loss: 0.6334 - val_binary_accuracy: 0.6111\n",
            "Epoch 175/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5727 - binary_accuracy: 0.7500\n",
            "Epoch 175: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4348 - binary_accuracy: 0.8019 - val_loss: 0.6359 - val_binary_accuracy: 0.6111\n",
            "Epoch 176/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4960 - binary_accuracy: 0.7812\n",
            "Epoch 176: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4154 - binary_accuracy: 0.8208 - val_loss: 0.6388 - val_binary_accuracy: 0.6111\n",
            "Epoch 177/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3847 - binary_accuracy: 0.8750\n",
            "Epoch 177: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4518 - binary_accuracy: 0.8302 - val_loss: 0.6388 - val_binary_accuracy: 0.6111\n",
            "Epoch 178/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4471 - binary_accuracy: 0.8125\n",
            "Epoch 178: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3970 - binary_accuracy: 0.8491 - val_loss: 0.6434 - val_binary_accuracy: 0.6111\n",
            "Epoch 179/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4212 - binary_accuracy: 0.7812\n",
            "Epoch 179: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.4215 - binary_accuracy: 0.8113 - val_loss: 0.6454 - val_binary_accuracy: 0.6111\n",
            "Epoch 180/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4049 - binary_accuracy: 0.8125\n",
            "Epoch 180: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4089 - binary_accuracy: 0.8208 - val_loss: 0.6485 - val_binary_accuracy: 0.6111\n",
            "Epoch 181/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4574 - binary_accuracy: 0.7812\n",
            "Epoch 181: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4152 - binary_accuracy: 0.7830 - val_loss: 0.6527 - val_binary_accuracy: 0.5833\n",
            "Epoch 182/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4190 - binary_accuracy: 0.8750\n",
            "Epoch 182: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3884 - binary_accuracy: 0.8774 - val_loss: 0.6546 - val_binary_accuracy: 0.5833\n",
            "Epoch 183/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4054 - binary_accuracy: 0.8438\n",
            "Epoch 183: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3656 - binary_accuracy: 0.8585 - val_loss: 0.6527 - val_binary_accuracy: 0.5833\n",
            "Epoch 184/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5920 - binary_accuracy: 0.6562\n",
            "Epoch 184: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3810 - binary_accuracy: 0.8396 - val_loss: 0.6535 - val_binary_accuracy: 0.5833\n",
            "Epoch 185/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3837 - binary_accuracy: 0.8750\n",
            "Epoch 185: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3953 - binary_accuracy: 0.8774 - val_loss: 0.6508 - val_binary_accuracy: 0.6111\n",
            "Epoch 186/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3956 - binary_accuracy: 0.8438\n",
            "Epoch 186: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3935 - binary_accuracy: 0.8396 - val_loss: 0.6561 - val_binary_accuracy: 0.6111\n",
            "Epoch 187/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4684 - binary_accuracy: 0.7500\n",
            "Epoch 187: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.3832 - binary_accuracy: 0.8491 - val_loss: 0.6596 - val_binary_accuracy: 0.6111\n",
            "Epoch 188/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2610 - binary_accuracy: 0.9688\n",
            "Epoch 188: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4006 - binary_accuracy: 0.8774 - val_loss: 0.6615 - val_binary_accuracy: 0.6111\n",
            "Epoch 189/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3487 - binary_accuracy: 0.8438\n",
            "Epoch 189: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.4076 - binary_accuracy: 0.8113 - val_loss: 0.6702 - val_binary_accuracy: 0.5833\n",
            "Epoch 190/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4059 - binary_accuracy: 0.7812\n",
            "Epoch 190: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3859 - binary_accuracy: 0.8396 - val_loss: 0.6753 - val_binary_accuracy: 0.5833\n",
            "Epoch 191/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4198 - binary_accuracy: 0.7500\n",
            "Epoch 191: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3987 - binary_accuracy: 0.8113 - val_loss: 0.6795 - val_binary_accuracy: 0.5833\n",
            "Epoch 192/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2934 - binary_accuracy: 0.8750\n",
            "Epoch 192: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3380 - binary_accuracy: 0.8679 - val_loss: 0.6741 - val_binary_accuracy: 0.6111\n",
            "Epoch 193/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4496 - binary_accuracy: 0.8438\n",
            "Epoch 193: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.4055 - binary_accuracy: 0.8679 - val_loss: 0.6737 - val_binary_accuracy: 0.6111\n",
            "Epoch 194/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3839 - binary_accuracy: 0.8438\n",
            "Epoch 194: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3842 - binary_accuracy: 0.8585 - val_loss: 0.6809 - val_binary_accuracy: 0.5833\n",
            "Epoch 195/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.5160 - binary_accuracy: 0.7500\n",
            "Epoch 195: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.4190 - binary_accuracy: 0.7925 - val_loss: 0.6875 - val_binary_accuracy: 0.6111\n",
            "Epoch 196/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4333 - binary_accuracy: 0.8125\n",
            "Epoch 196: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3706 - binary_accuracy: 0.8585 - val_loss: 0.6948 - val_binary_accuracy: 0.6111\n",
            "Epoch 197/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3265 - binary_accuracy: 0.9062\n",
            "Epoch 197: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.3503 - binary_accuracy: 0.8962 - val_loss: 0.7000 - val_binary_accuracy: 0.6111\n",
            "Epoch 198/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4598 - binary_accuracy: 0.7812\n",
            "Epoch 198: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3997 - binary_accuracy: 0.8491 - val_loss: 0.7023 - val_binary_accuracy: 0.6111\n",
            "Epoch 199/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3920 - binary_accuracy: 0.8125\n",
            "Epoch 199: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.3655 - binary_accuracy: 0.8679 - val_loss: 0.7024 - val_binary_accuracy: 0.6111\n",
            "Epoch 200/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4555 - binary_accuracy: 0.8125\n",
            "Epoch 200: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.3533 - binary_accuracy: 0.8868 - val_loss: 0.7020 - val_binary_accuracy: 0.6111\n",
            "Epoch 201/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4496 - binary_accuracy: 0.7500\n",
            "Epoch 201: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.3420 - binary_accuracy: 0.8679 - val_loss: 0.7000 - val_binary_accuracy: 0.6111\n",
            "Epoch 202/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2544 - binary_accuracy: 0.9688\n",
            "Epoch 202: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2994 - binary_accuracy: 0.8962 - val_loss: 0.7027 - val_binary_accuracy: 0.6111\n",
            "Epoch 203/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4003 - binary_accuracy: 0.7812\n",
            "Epoch 203: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.3881 - binary_accuracy: 0.8208 - val_loss: 0.7115 - val_binary_accuracy: 0.6111\n",
            "Epoch 204/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3570 - binary_accuracy: 0.8750\n",
            "Epoch 204: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.3707 - binary_accuracy: 0.8774 - val_loss: 0.7211 - val_binary_accuracy: 0.6111\n",
            "Epoch 205/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4690 - binary_accuracy: 0.7812\n",
            "Epoch 205: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.3891 - binary_accuracy: 0.8774 - val_loss: 0.7279 - val_binary_accuracy: 0.6111\n",
            "Epoch 206/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3838 - binary_accuracy: 0.8750\n",
            "Epoch 206: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3417 - binary_accuracy: 0.8679 - val_loss: 0.7298 - val_binary_accuracy: 0.6111\n",
            "Epoch 207/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3604 - binary_accuracy: 0.8438\n",
            "Epoch 207: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.3603 - binary_accuracy: 0.8679 - val_loss: 0.7321 - val_binary_accuracy: 0.6111\n",
            "Epoch 208/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3841 - binary_accuracy: 0.8438\n",
            "Epoch 208: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.3375 - binary_accuracy: 0.8774 - val_loss: 0.7318 - val_binary_accuracy: 0.6111\n",
            "Epoch 209/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3871 - binary_accuracy: 0.8438\n",
            "Epoch 209: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3682 - binary_accuracy: 0.8585 - val_loss: 0.7205 - val_binary_accuracy: 0.6111\n",
            "Epoch 210/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3531 - binary_accuracy: 0.8750\n",
            "Epoch 210: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3418 - binary_accuracy: 0.8962 - val_loss: 0.7180 - val_binary_accuracy: 0.6111\n",
            "Epoch 211/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4899 - binary_accuracy: 0.8125\n",
            "Epoch 211: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3833 - binary_accuracy: 0.8491 - val_loss: 0.7182 - val_binary_accuracy: 0.6111\n",
            "Epoch 212/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3675 - binary_accuracy: 0.8750\n",
            "Epoch 212: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3211 - binary_accuracy: 0.8868 - val_loss: 0.7271 - val_binary_accuracy: 0.6111\n",
            "Epoch 213/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3762 - binary_accuracy: 0.8438\n",
            "Epoch 213: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.3494 - binary_accuracy: 0.8679 - val_loss: 0.7346 - val_binary_accuracy: 0.6111\n",
            "Epoch 214/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3418 - binary_accuracy: 0.8438\n",
            "Epoch 214: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3231 - binary_accuracy: 0.8774 - val_loss: 0.7387 - val_binary_accuracy: 0.6111\n",
            "Epoch 215/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3692 - binary_accuracy: 0.8125\n",
            "Epoch 215: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.3321 - binary_accuracy: 0.8868 - val_loss: 0.7413 - val_binary_accuracy: 0.6111\n",
            "Epoch 216/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2776 - binary_accuracy: 0.9062\n",
            "Epoch 216: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3315 - binary_accuracy: 0.8396 - val_loss: 0.7438 - val_binary_accuracy: 0.6111\n",
            "Epoch 217/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2985 - binary_accuracy: 0.9062\n",
            "Epoch 217: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2877 - binary_accuracy: 0.8962 - val_loss: 0.7498 - val_binary_accuracy: 0.6111\n",
            "Epoch 218/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3311 - binary_accuracy: 0.9062\n",
            "Epoch 218: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3365 - binary_accuracy: 0.8868 - val_loss: 0.7574 - val_binary_accuracy: 0.6111\n",
            "Epoch 219/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2482 - binary_accuracy: 0.8750\n",
            "Epoch 219: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3015 - binary_accuracy: 0.8585 - val_loss: 0.7653 - val_binary_accuracy: 0.6111\n",
            "Epoch 220/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3863 - binary_accuracy: 0.8438\n",
            "Epoch 220: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3160 - binary_accuracy: 0.8491 - val_loss: 0.7801 - val_binary_accuracy: 0.5556\n",
            "Epoch 221/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3547 - binary_accuracy: 0.8750\n",
            "Epoch 221: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3214 - binary_accuracy: 0.8774 - val_loss: 0.7900 - val_binary_accuracy: 0.5556\n",
            "Epoch 222/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3255 - binary_accuracy: 0.9062\n",
            "Epoch 222: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.3406 - binary_accuracy: 0.8679 - val_loss: 0.7954 - val_binary_accuracy: 0.5833\n",
            "Epoch 223/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2822 - binary_accuracy: 0.9062\n",
            "Epoch 223: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3525 - binary_accuracy: 0.8774 - val_loss: 0.7999 - val_binary_accuracy: 0.5833\n",
            "Epoch 224/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2347 - binary_accuracy: 0.9062\n",
            "Epoch 224: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2954 - binary_accuracy: 0.8585 - val_loss: 0.8016 - val_binary_accuracy: 0.5833\n",
            "Epoch 225/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2950 - binary_accuracy: 0.8750\n",
            "Epoch 225: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3120 - binary_accuracy: 0.8774 - val_loss: 0.7950 - val_binary_accuracy: 0.5833\n",
            "Epoch 226/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3585 - binary_accuracy: 0.9062\n",
            "Epoch 226: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3069 - binary_accuracy: 0.9057 - val_loss: 0.7823 - val_binary_accuracy: 0.6111\n",
            "Epoch 227/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3378 - binary_accuracy: 0.8750\n",
            "Epoch 227: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3927 - binary_accuracy: 0.8491 - val_loss: 0.7732 - val_binary_accuracy: 0.6111\n",
            "Epoch 228/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2673 - binary_accuracy: 0.8750\n",
            "Epoch 228: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 32ms/step - loss: 0.3389 - binary_accuracy: 0.8774 - val_loss: 0.7692 - val_binary_accuracy: 0.6111\n",
            "Epoch 229/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4602 - binary_accuracy: 0.8438\n",
            "Epoch 229: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.3687 - binary_accuracy: 0.8396 - val_loss: 0.7793 - val_binary_accuracy: 0.5833\n",
            "Epoch 230/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3545 - binary_accuracy: 0.8125\n",
            "Epoch 230: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3314 - binary_accuracy: 0.8491 - val_loss: 0.7975 - val_binary_accuracy: 0.5833\n",
            "Epoch 231/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3132 - binary_accuracy: 0.9062\n",
            "Epoch 231: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3154 - binary_accuracy: 0.8585 - val_loss: 0.8104 - val_binary_accuracy: 0.5556\n",
            "Epoch 232/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3013 - binary_accuracy: 0.9062\n",
            "Epoch 232: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.3279 - binary_accuracy: 0.8774 - val_loss: 0.8076 - val_binary_accuracy: 0.5556\n",
            "Epoch 233/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3813 - binary_accuracy: 0.8125\n",
            "Epoch 233: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3109 - binary_accuracy: 0.8774 - val_loss: 0.8112 - val_binary_accuracy: 0.5556\n",
            "Epoch 234/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2606 - binary_accuracy: 0.8750\n",
            "Epoch 234: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3231 - binary_accuracy: 0.8868 - val_loss: 0.8202 - val_binary_accuracy: 0.5556\n",
            "Epoch 235/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2910 - binary_accuracy: 0.9375\n",
            "Epoch 235: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3480 - binary_accuracy: 0.8774 - val_loss: 0.8362 - val_binary_accuracy: 0.5556\n",
            "Epoch 236/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2549 - binary_accuracy: 0.9062\n",
            "Epoch 236: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2924 - binary_accuracy: 0.8962 - val_loss: 0.8415 - val_binary_accuracy: 0.5556\n",
            "Epoch 237/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2425 - binary_accuracy: 0.9375\n",
            "Epoch 237: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.2570 - binary_accuracy: 0.9057 - val_loss: 0.8397 - val_binary_accuracy: 0.5556\n",
            "Epoch 238/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3821 - binary_accuracy: 0.8438\n",
            "Epoch 238: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3262 - binary_accuracy: 0.8585 - val_loss: 0.8368 - val_binary_accuracy: 0.5556\n",
            "Epoch 239/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2307 - binary_accuracy: 0.9375\n",
            "Epoch 239: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2858 - binary_accuracy: 0.8868 - val_loss: 0.8449 - val_binary_accuracy: 0.5556\n",
            "Epoch 240/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2729 - binary_accuracy: 0.9375\n",
            "Epoch 240: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3082 - binary_accuracy: 0.9340 - val_loss: 0.8547 - val_binary_accuracy: 0.5556\n",
            "Epoch 241/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3080 - binary_accuracy: 0.8438\n",
            "Epoch 241: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3486 - binary_accuracy: 0.8491 - val_loss: 0.8671 - val_binary_accuracy: 0.5556\n",
            "Epoch 242/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2855 - binary_accuracy: 0.9375\n",
            "Epoch 242: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.3295 - binary_accuracy: 0.9245 - val_loss: 0.8717 - val_binary_accuracy: 0.5556\n",
            "Epoch 243/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3172 - binary_accuracy: 0.8750\n",
            "Epoch 243: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2911 - binary_accuracy: 0.8774 - val_loss: 0.8653 - val_binary_accuracy: 0.5556\n",
            "Epoch 244/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2768 - binary_accuracy: 0.9375\n",
            "Epoch 244: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 33ms/step - loss: 0.2526 - binary_accuracy: 0.9245 - val_loss: 0.8621 - val_binary_accuracy: 0.5556\n",
            "Epoch 245/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2592 - binary_accuracy: 0.9062\n",
            "Epoch 245: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.3087 - binary_accuracy: 0.8585 - val_loss: 0.8487 - val_binary_accuracy: 0.5556\n",
            "Epoch 246/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2961 - binary_accuracy: 0.9688\n",
            "Epoch 246: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3454 - binary_accuracy: 0.8962 - val_loss: 0.8390 - val_binary_accuracy: 0.5556\n",
            "Epoch 247/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2856 - binary_accuracy: 0.8125\n",
            "Epoch 247: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2876 - binary_accuracy: 0.8679 - val_loss: 0.8362 - val_binary_accuracy: 0.5556\n",
            "Epoch 248/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3281 - binary_accuracy: 0.9062\n",
            "Epoch 248: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2770 - binary_accuracy: 0.9245 - val_loss: 0.8314 - val_binary_accuracy: 0.5833\n",
            "Epoch 249/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2160 - binary_accuracy: 0.9375\n",
            "Epoch 249: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2666 - binary_accuracy: 0.9340 - val_loss: 0.8310 - val_binary_accuracy: 0.5833\n",
            "Epoch 250/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1661 - binary_accuracy: 0.9688\n",
            "Epoch 250: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 36ms/step - loss: 0.2661 - binary_accuracy: 0.9245 - val_loss: 0.8313 - val_binary_accuracy: 0.5833\n",
            "Epoch 251/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3558 - binary_accuracy: 0.8438\n",
            "Epoch 251: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3065 - binary_accuracy: 0.9057 - val_loss: 0.8384 - val_binary_accuracy: 0.5833\n",
            "Epoch 252/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3103 - binary_accuracy: 0.9375\n",
            "Epoch 252: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3236 - binary_accuracy: 0.9057 - val_loss: 0.8569 - val_binary_accuracy: 0.5833\n",
            "Epoch 253/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2121 - binary_accuracy: 0.9688\n",
            "Epoch 253: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2750 - binary_accuracy: 0.8962 - val_loss: 0.8727 - val_binary_accuracy: 0.5556\n",
            "Epoch 254/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3614 - binary_accuracy: 0.8438\n",
            "Epoch 254: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2904 - binary_accuracy: 0.8679 - val_loss: 0.8815 - val_binary_accuracy: 0.5556\n",
            "Epoch 255/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3105 - binary_accuracy: 0.8438\n",
            "Epoch 255: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.2997 - binary_accuracy: 0.8585 - val_loss: 0.8784 - val_binary_accuracy: 0.5833\n",
            "Epoch 256/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3288 - binary_accuracy: 0.8750\n",
            "Epoch 256: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3044 - binary_accuracy: 0.8774 - val_loss: 0.8814 - val_binary_accuracy: 0.5833\n",
            "Epoch 257/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3384 - binary_accuracy: 0.9062\n",
            "Epoch 257: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.3154 - binary_accuracy: 0.8962 - val_loss: 0.8828 - val_binary_accuracy: 0.5833\n",
            "Epoch 258/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2556 - binary_accuracy: 0.9375\n",
            "Epoch 258: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.2120 - binary_accuracy: 0.9528 - val_loss: 0.8837 - val_binary_accuracy: 0.5833\n",
            "Epoch 259/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2561 - binary_accuracy: 0.9375\n",
            "Epoch 259: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.3061 - binary_accuracy: 0.9151 - val_loss: 0.8823 - val_binary_accuracy: 0.5833\n",
            "Epoch 260/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2667 - binary_accuracy: 0.8438\n",
            "Epoch 260: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 29ms/step - loss: 0.3032 - binary_accuracy: 0.8585 - val_loss: 0.8895 - val_binary_accuracy: 0.5833\n",
            "Epoch 261/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2388 - binary_accuracy: 0.9375\n",
            "Epoch 261: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.2830 - binary_accuracy: 0.9151 - val_loss: 0.8897 - val_binary_accuracy: 0.5833\n",
            "Epoch 262/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2222 - binary_accuracy: 0.8750\n",
            "Epoch 262: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 30ms/step - loss: 0.2765 - binary_accuracy: 0.8868 - val_loss: 0.8919 - val_binary_accuracy: 0.5833\n",
            "Epoch 263/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3265 - binary_accuracy: 0.8750\n",
            "Epoch 263: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 31ms/step - loss: 0.3189 - binary_accuracy: 0.8868 - val_loss: 0.8938 - val_binary_accuracy: 0.5833\n",
            "Epoch 264/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3813 - binary_accuracy: 0.7812\n",
            "Epoch 264: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2746 - binary_accuracy: 0.8774 - val_loss: 0.8955 - val_binary_accuracy: 0.5833\n",
            "Epoch 265/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2640 - binary_accuracy: 0.9375\n",
            "Epoch 265: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 35ms/step - loss: 0.2693 - binary_accuracy: 0.9151 - val_loss: 0.8942 - val_binary_accuracy: 0.5833\n",
            "Epoch 266/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2905 - binary_accuracy: 0.8750\n",
            "Epoch 266: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3063 - binary_accuracy: 0.8585 - val_loss: 0.8985 - val_binary_accuracy: 0.5833\n",
            "Epoch 267/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4028 - binary_accuracy: 0.8125\n",
            "Epoch 267: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2440 - binary_accuracy: 0.9151 - val_loss: 0.8944 - val_binary_accuracy: 0.5833\n",
            "Epoch 268/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1805 - binary_accuracy: 0.9375\n",
            "Epoch 268: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2505 - binary_accuracy: 0.9151 - val_loss: 0.8869 - val_binary_accuracy: 0.5833\n",
            "Epoch 269/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2288 - binary_accuracy: 0.9062\n",
            "Epoch 269: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.3341 - binary_accuracy: 0.8491 - val_loss: 0.8887 - val_binary_accuracy: 0.5833\n",
            "Epoch 270/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3316 - binary_accuracy: 0.8438\n",
            "Epoch 270: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 28ms/step - loss: 0.2470 - binary_accuracy: 0.9151 - val_loss: 0.8914 - val_binary_accuracy: 0.5833\n",
            "Epoch 271/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2107 - binary_accuracy: 0.8750\n",
            "Epoch 271: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2457 - binary_accuracy: 0.8962 - val_loss: 0.8929 - val_binary_accuracy: 0.5833\n",
            "Epoch 272/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1561 - binary_accuracy: 1.0000\n",
            "Epoch 272: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2860 - binary_accuracy: 0.9057 - val_loss: 0.9004 - val_binary_accuracy: 0.5833\n",
            "Epoch 273/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3523 - binary_accuracy: 0.8438\n",
            "Epoch 273: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2858 - binary_accuracy: 0.8868 - val_loss: 0.9027 - val_binary_accuracy: 0.5833\n",
            "Epoch 274/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2936 - binary_accuracy: 0.8750\n",
            "Epoch 274: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2518 - binary_accuracy: 0.9057 - val_loss: 0.9035 - val_binary_accuracy: 0.5833\n",
            "Epoch 275/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2573 - binary_accuracy: 0.9062\n",
            "Epoch 275: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.3484 - binary_accuracy: 0.8396 - val_loss: 0.9044 - val_binary_accuracy: 0.5833\n",
            "Epoch 276/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2489 - binary_accuracy: 0.9062\n",
            "Epoch 276: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.2312 - binary_accuracy: 0.9434 - val_loss: 0.9031 - val_binary_accuracy: 0.5833\n",
            "Epoch 277/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2456 - binary_accuracy: 0.9062\n",
            "Epoch 277: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2539 - binary_accuracy: 0.9151 - val_loss: 0.8949 - val_binary_accuracy: 0.5833\n",
            "Epoch 278/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3743 - binary_accuracy: 0.8750\n",
            "Epoch 278: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.3002 - binary_accuracy: 0.8962 - val_loss: 0.8920 - val_binary_accuracy: 0.5833\n",
            "Epoch 279/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4033 - binary_accuracy: 0.7812\n",
            "Epoch 279: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2599 - binary_accuracy: 0.8868 - val_loss: 0.8917 - val_binary_accuracy: 0.5833\n",
            "Epoch 280/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4550 - binary_accuracy: 0.8438\n",
            "Epoch 280: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2862 - binary_accuracy: 0.8962 - val_loss: 0.8971 - val_binary_accuracy: 0.5833\n",
            "Epoch 281/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2363 - binary_accuracy: 0.9375\n",
            "Epoch 281: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2884 - binary_accuracy: 0.8679 - val_loss: 0.9087 - val_binary_accuracy: 0.5833\n",
            "Epoch 282/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3986 - binary_accuracy: 0.8438\n",
            "Epoch 282: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2748 - binary_accuracy: 0.8962 - val_loss: 0.9203 - val_binary_accuracy: 0.5833\n",
            "Epoch 283/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2487 - binary_accuracy: 0.9062\n",
            "Epoch 283: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2831 - binary_accuracy: 0.9057 - val_loss: 0.9227 - val_binary_accuracy: 0.5833\n",
            "Epoch 284/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2836 - binary_accuracy: 0.9062\n",
            "Epoch 284: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2814 - binary_accuracy: 0.9151 - val_loss: 0.9226 - val_binary_accuracy: 0.5833\n",
            "Epoch 285/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4265 - binary_accuracy: 0.7500\n",
            "Epoch 285: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3133 - binary_accuracy: 0.8585 - val_loss: 0.9196 - val_binary_accuracy: 0.5833\n",
            "Epoch 286/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2303 - binary_accuracy: 0.9062\n",
            "Epoch 286: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.2427 - binary_accuracy: 0.9245 - val_loss: 0.9128 - val_binary_accuracy: 0.5833\n",
            "Epoch 287/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1462 - binary_accuracy: 0.9688\n",
            "Epoch 287: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2750 - binary_accuracy: 0.9151 - val_loss: 0.9133 - val_binary_accuracy: 0.5833\n",
            "Epoch 288/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1234 - binary_accuracy: 0.9688\n",
            "Epoch 288: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2695 - binary_accuracy: 0.8868 - val_loss: 0.9160 - val_binary_accuracy: 0.5833\n",
            "Epoch 289/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4653 - binary_accuracy: 0.8125\n",
            "Epoch 289: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2969 - binary_accuracy: 0.9151 - val_loss: 0.9230 - val_binary_accuracy: 0.5833\n",
            "Epoch 290/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2733 - binary_accuracy: 0.8750\n",
            "Epoch 290: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2774 - binary_accuracy: 0.8868 - val_loss: 0.9286 - val_binary_accuracy: 0.5833\n",
            "Epoch 291/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2895 - binary_accuracy: 0.9062\n",
            "Epoch 291: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2288 - binary_accuracy: 0.9245 - val_loss: 0.9329 - val_binary_accuracy: 0.5833\n",
            "Epoch 292/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2035 - binary_accuracy: 0.9375\n",
            "Epoch 292: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2373 - binary_accuracy: 0.9057 - val_loss: 0.9409 - val_binary_accuracy: 0.5833\n",
            "Epoch 293/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1601 - binary_accuracy: 0.9375\n",
            "Epoch 293: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2952 - binary_accuracy: 0.8491 - val_loss: 0.9511 - val_binary_accuracy: 0.5833\n",
            "Epoch 294/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2656 - binary_accuracy: 0.8750\n",
            "Epoch 294: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2786 - binary_accuracy: 0.9057 - val_loss: 0.9582 - val_binary_accuracy: 0.5833\n",
            "Epoch 295/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3538 - binary_accuracy: 0.8438\n",
            "Epoch 295: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2594 - binary_accuracy: 0.8962 - val_loss: 0.9580 - val_binary_accuracy: 0.5833\n",
            "Epoch 296/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2903 - binary_accuracy: 0.9062\n",
            "Epoch 296: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2488 - binary_accuracy: 0.9245 - val_loss: 0.9709 - val_binary_accuracy: 0.5833\n",
            "Epoch 297/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3592 - binary_accuracy: 0.8438\n",
            "Epoch 297: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2615 - binary_accuracy: 0.9151 - val_loss: 0.9777 - val_binary_accuracy: 0.5833\n",
            "Epoch 298/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2271 - binary_accuracy: 0.8750\n",
            "Epoch 298: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.2619 - binary_accuracy: 0.8774 - val_loss: 0.9796 - val_binary_accuracy: 0.5833\n",
            "Epoch 299/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2623 - binary_accuracy: 0.9062\n",
            "Epoch 299: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2696 - binary_accuracy: 0.9057 - val_loss: 0.9713 - val_binary_accuracy: 0.5833\n",
            "Epoch 300/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3677 - binary_accuracy: 0.8750\n",
            "Epoch 300: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2359 - binary_accuracy: 0.9057 - val_loss: 0.9715 - val_binary_accuracy: 0.5833\n",
            "Epoch 301/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4065 - binary_accuracy: 0.8750\n",
            "Epoch 301: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2976 - binary_accuracy: 0.9151 - val_loss: 0.9700 - val_binary_accuracy: 0.5833\n",
            "Epoch 302/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3598 - binary_accuracy: 0.8438\n",
            "Epoch 302: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2524 - binary_accuracy: 0.9057 - val_loss: 0.9634 - val_binary_accuracy: 0.5833\n",
            "Epoch 303/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2298 - binary_accuracy: 0.9062\n",
            "Epoch 303: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2680 - binary_accuracy: 0.9057 - val_loss: 0.9614 - val_binary_accuracy: 0.5833\n",
            "Epoch 304/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1714 - binary_accuracy: 0.9688\n",
            "Epoch 304: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2448 - binary_accuracy: 0.9057 - val_loss: 0.9662 - val_binary_accuracy: 0.5833\n",
            "Epoch 305/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2311 - binary_accuracy: 0.9375\n",
            "Epoch 305: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 26ms/step - loss: 0.1789 - binary_accuracy: 0.9434 - val_loss: 0.9775 - val_binary_accuracy: 0.5833\n",
            "Epoch 306/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2447 - binary_accuracy: 0.8438\n",
            "Epoch 306: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.2914 - binary_accuracy: 0.8396 - val_loss: 0.9813 - val_binary_accuracy: 0.5833\n",
            "Epoch 307/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3560 - binary_accuracy: 0.8750\n",
            "Epoch 307: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.2427 - binary_accuracy: 0.8962 - val_loss: 0.9730 - val_binary_accuracy: 0.5833\n",
            "Epoch 308/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2299 - binary_accuracy: 0.9375\n",
            "Epoch 308: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2196 - binary_accuracy: 0.9340 - val_loss: 0.9774 - val_binary_accuracy: 0.5833\n",
            "Epoch 309/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3704 - binary_accuracy: 0.7812\n",
            "Epoch 309: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2531 - binary_accuracy: 0.8962 - val_loss: 0.9688 - val_binary_accuracy: 0.5833\n",
            "Epoch 310/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2195 - binary_accuracy: 0.9062\n",
            "Epoch 310: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2775 - binary_accuracy: 0.8868 - val_loss: 0.9583 - val_binary_accuracy: 0.5833\n",
            "Epoch 311/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3081 - binary_accuracy: 0.9062\n",
            "Epoch 311: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 21ms/step - loss: 0.2752 - binary_accuracy: 0.9057 - val_loss: 0.9442 - val_binary_accuracy: 0.5833\n",
            "Epoch 312/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2012 - binary_accuracy: 0.9375\n",
            "Epoch 312: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2105 - binary_accuracy: 0.9340 - val_loss: 0.9498 - val_binary_accuracy: 0.5833\n",
            "Epoch 313/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3127 - binary_accuracy: 0.8438\n",
            "Epoch 313: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.3115 - binary_accuracy: 0.8868 - val_loss: 0.9665 - val_binary_accuracy: 0.5833\n",
            "Epoch 314/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1597 - binary_accuracy: 0.9688\n",
            "Epoch 314: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 18ms/step - loss: 0.2366 - binary_accuracy: 0.9245 - val_loss: 0.9917 - val_binary_accuracy: 0.5833\n",
            "Epoch 315/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2244 - binary_accuracy: 0.9375\n",
            "Epoch 315: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 20ms/step - loss: 0.2519 - binary_accuracy: 0.9057 - val_loss: 1.0053 - val_binary_accuracy: 0.5833\n",
            "Epoch 316/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.4045 - binary_accuracy: 0.8125\n",
            "Epoch 316: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2225 - binary_accuracy: 0.9245 - val_loss: 1.0105 - val_binary_accuracy: 0.5833\n",
            "Epoch 317/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2242 - binary_accuracy: 0.9062\n",
            "Epoch 317: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 25ms/step - loss: 0.2419 - binary_accuracy: 0.9151 - val_loss: 1.0084 - val_binary_accuracy: 0.5833\n",
            "Epoch 318/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2641 - binary_accuracy: 0.9062\n",
            "Epoch 318: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 23ms/step - loss: 0.2246 - binary_accuracy: 0.9340 - val_loss: 1.0017 - val_binary_accuracy: 0.5833\n",
            "Epoch 319/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.2298 - binary_accuracy: 0.9375\n",
            "Epoch 319: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2813 - binary_accuracy: 0.9057 - val_loss: 0.9923 - val_binary_accuracy: 0.5833\n",
            "Epoch 320/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1689 - binary_accuracy: 1.0000\n",
            "Epoch 320: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.2379 - binary_accuracy: 0.9151 - val_loss: 0.9857 - val_binary_accuracy: 0.5833\n",
            "Epoch 321/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1401 - binary_accuracy: 0.9375\n",
            "Epoch 321: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 22ms/step - loss: 0.2300 - binary_accuracy: 0.9340 - val_loss: 1.0095 - val_binary_accuracy: 0.5833\n",
            "Epoch 322/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3009 - binary_accuracy: 0.9062\n",
            "Epoch 322: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 24ms/step - loss: 0.2278 - binary_accuracy: 0.9340 - val_loss: 1.0290 - val_binary_accuracy: 0.5833\n",
            "Epoch 323/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.3614 - binary_accuracy: 0.8438\n",
            "Epoch 323: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 19ms/step - loss: 0.3043 - binary_accuracy: 0.8679 - val_loss: 1.0376 - val_binary_accuracy: 0.5833\n",
            "Epoch 324/5000\n",
            "1/4 [======>.......................] - ETA: 0s - loss: 0.1607 - binary_accuracy: 0.9688\n",
            "Epoch 324: val_binary_accuracy did not improve from 0.75000\n",
            "4/4 [==============================] - 0s 27ms/step - loss: 0.1906 - binary_accuracy: 0.9434 - val_loss: 1.0325 - val_binary_accuracy: 0.5833\n",
            "Epoch 324: early stopping\n",
            "Train Time: 34.19963502883911 seconds\n"
          ]
        }
      ],
      "source": [
        "# LSTM Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))) # stacked LSTM\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(32, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))) # stacked LSTM\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['binary_accuracy'])\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Early Stopping\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "\n",
        "# Model Checkpoint\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5000, batch_size=32, callbacks=[es, mc])\n",
        "\n",
        "print(\"Train Time: %s seconds\" % (time.time() - start_time))\n",
        "\n",
        "# traintime = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved model\n",
        "saved_model = load_model('best_model.h5')\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "metadata": {
        "id": "VqKySRDCYpe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4a3f5d-55e8-4743-ea4d-1b917278cbed"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 0.953, Test: 0.583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model = load_model('best_model.h5')"
      ],
      "metadata": {
        "id": "2XpUmPoLcLzc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot training history\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.title(\"Training History\")\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "EHICzRKeYyS8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "34985824-2e53-498f-c6de-b3305013152a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _replaceitem(x):\n",
        "    if x > 0.1:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "WxuRSEL9Tkfv"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "0G5ihpX75Hyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498a4127-5d36-49f1-e1d2-efd5cf4c6cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 6ms/step\n",
            "Test Time: 1.374776840209961 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "y_pred = saved_model.predict(X_test)\n",
        "\n",
        "# print(\"Train Time: %f seconds\" % traintime)\n",
        "print(\"Test Time: %s seconds\" % (time.time() - start_time))\n",
        "\n",
        "y_pred = pd.DataFrame({'y_pred': y_pred.round().ravel()})\n",
        "# y_pred = list(map(_replaceitem, y_pred.ravel()))\n",
        "results = pd.concat([y_test,y_pred], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "OBpLLIkW64GR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7df5a4d-d118-4798-ea51-cc5fd0fccf65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 7 2 23\n"
          ]
        }
      ],
      "source": [
        "if(len(confusion_matrix(y_test, y_pred).ravel()) > 1):\n",
        "  tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
        "  print(tn, fp, fn, tp)\n",
        "else:\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  print(cm)\n",
        "ac = accuracy_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "NCBs8iri9on1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adca50b1-868c-4113-ff6e-326eb1163291"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.75"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "ac"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')\n",
        "\n",
        "# model.save('lstmplus.h5')"
      ],
      "metadata": {
        "id": "N_wKivhzuZX0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P-D8MGDBzlR4"
      },
      "execution_count": 74,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6OcAcizFRCvI",
        "mccVGRxOfKQv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}